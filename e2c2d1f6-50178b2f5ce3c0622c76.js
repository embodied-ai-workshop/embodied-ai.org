"use strict";(self.webpackChunkembodied_ai_workshop=self.webpackChunkembodied_ai_workshop||[]).push([[156],{9423:function(e,a,t){t.r(a),t.d(a,{default:function(){return Z}});t(3960);var i=t(1381),n=(t(1651),t(9304)),o=t(436),r=(t(9412),t(7262)),s=t(6540),l=t(6568),c=t(3020),d=t(2316),u=t(2158),p=t(4743),g=t.n(p),h=t(4333),m=t(9567),f=t(853),v=t(7850),b=t(1013);const{Step:w}=r.A;function y(e){return(0,b.Y)(v.Video,{fontSize:"45px",url:e.url},(0,b.Y)(h.A,{fluid:e.data[e.imageQuery].childImageSharp.fluid}))}function Y(e){return(0,b.Y)("div",{css:(0,b.AH)("width:",e.width?e.width:"175px",";margin-bottom:12px;display:",e.display?e.display:"inline-block",";text-align:center;margin-right:","block"===e.display?"auto":"4px",";margin-left:","block"===e.display?"auto":"4px",";","","","")},(0,b.Y)(v.Video,{fontSize:e.playSize?e.playSize:"25px",url:e.url},(0,b.Y)(h.A,{fluid:e.data[e.imageQuery].childImageSharp.fluid})),(0,b.Y)("div",{css:(0,b.AH)("background-color:",d.A.gray4,";border-radius:0px 0px 3px 3px;border-right:1px solid ",d.A.gray6,";border-left:1px solid ",d.A.gray6,";border-bottom:1px solid ",d.A.gray6,";","","","")},e.rank))}const k={"AI2-THOR ObjectNav":(0,b.Y)("a",{href:"//ai2thor.allenai.org/robothor/cvpr-2021-challenge",target:"_blank"},"AI2-THOR ObjectNav"),"AI2-THOR Rearrangement":(0,b.Y)("a",{href:"//ai2thor.allenai.org/rearrangement",target:"_blank"},"AI2-THOR Rearrangement"),ALFRED:(0,b.Y)("a",{href:"//askforalfred.com/EAI21/",target:"_blank"},"ALFRED"),Habitat:(0,b.Y)("a",{href:"//aihabitat.org/challenge/2021",target:"_blank"},"Habitat"),iGibson:(0,b.Y)("a",{href:"http://svl.stanford.edu/igibson/challenge.html",target:"_blank"},"iGibson"),MultiOn:(0,b.Y)("a",{href:"//multion-challenge.github.io/",target:"_blank"},"MultiON"),"Robotic Vision Scene Understanding":(0,b.Y)("a",{href:"http://cvpr2021.roboticvisionchallenge.org/",target:"_blank"},"Robotic Vision Scene Understanding"),"RxR-Habitat":(0,b.Y)("a",{href:"//ai.google.com/research/rxr/habitat",target:"_blank"},"RxR-Habitat"),SoundSpaces:(0,b.Y)("a",{href:"//soundspaces.org/challenge",target:"_blank"},"SoundSpaces"),"TDW-Transport":(0,b.Y)("a",{href:"http://tdw-transport.csail.mit.edu/",target:"_blank"},"TDW-Transport")};var A={name:"14y6t8x",styles:"display:none!important"},x={name:"99y604",styles:"margin-top:5px;color:#8c8c8c"},P={name:"rowx5j",styles:"border-radius:5px;box-shadow:0px 0px 2px 0px #2b4acb;display:inline-block;margin:auto;*{padding-top:3px;padding-bottom:5px;}"},S={name:"nl3i1y",styles:"vertical-align:middle;display:inline-block;margin-top:6px;margin-left:5px"},L={name:"13sa3rf",styles:'font-weight:bold;font-size:25px;color:"#2b4acb";vertical-align:middle;display:inline-block'},I={name:"pr10xp",styles:"margin-bottom:10px"},D={name:"uewfz3",styles:"text-align:center;margin-top:60px;margin-bottom:60px"};function z(e){const{0:a,1:t}=(0,s.useState)(!1),{0:i,1:n}=(0,s.useState)(!1),{0:o,1:r}=(0,s.useState)(""),l=/^(([^<>()[\]\\.,;:\s@"]+(\.[^<>()[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/.test(String(o).toLowerCase());return(0,b.Y)("div",{css:D},(0,b.Y)("form",{encType:"text/plain",action:l?"https://docs.google.com/forms/d/e/"+e.actionIdentifier+"/formResponse?usp=pp_url&entry."+e.entryNumber+"="+o:"",target:"hidden_iframe"+e.actionIdentifier,onSubmit:()=>!!l&&t(!0),method:"post"},(0,b.Y)("div",{css:I},(0,b.Y)("div",{css:L},"Sign Up for Updates"),(0,b.Y)("div",{css:S})),a?(0,b.Y)("div",null,"Thanks for signing up!"):(0,b.Y)(s.Fragment,null,(0,b.Y)("div",{css:P},(0,b.Y)("input",{type:"email",autoComplete:"off",placeholder:"email",name:"entry."+e.entryNumber,id:"entry."+e.entryNumber,onFocus:()=>n(!0),onBlur:()=>n(!1),onChange:e=>r(e.target.value),value:o,css:(0,b.AH)("background-color:transparent;transition-duration:0.3s;box-shadow:0px 0px 1px 2px ",i||l||""==o?"transparent":"#ff7875",";border:none;width:350px;@media (max-width: 500px){width:55vw;}border-radius:5px;padding-left:8px;","","","")}),(0,b.Y)("input",{type:l?"submit":"button",value:"Sign Up",onClick:()=>!!l,css:(0,b.AH)("background-color:transparent;border:none;font-weight:600;transition-duration:0.3s;color:",l?"#2b4acb":"#2b4acb88",";padding-top:3px;padding-right:12px;padding-left:10px;&:hover{cursor:",l?"pointer":"default",";}","","","")})),(0,b.Y)("div",{css:x},"You can unsubscribe at any time."))),(0,b.Y)("iframe",{name:"hidden_iframe"+e.actionIdentifier,id:"hidden_iframe"+e.actionIdentifier,css:A}))}function M(){if("undefined"==typeof window)return 800;const{innerWidth:e}=window;return e}var R={name:"nkt64x",styles:"margin-right:10px"};function T(e){return(0,b.Y)("a",{href:e.url,target:"_blank",css:R},(0,b.Y)("div",{css:(0,b.AH)("display:inline-block;border:1px solid ",d.A.gray5,";background-color:",d.A.gray2,";padding-left:7px;padding-right:7px;border-radius:5px;transition-duration:0.15s;>span{vertical-align:middle;}&:hover{background-color:",d.A.gray4,";border:1px solid ",d.A.gray6,";}","","","")},(0,b.Y)("span",{css:(0,b.AH)("margin-left:5px;color:",d.A.gray10,";","","","")},e.text)))}var X={name:"4ozr74",styles:"position:absolute;bottom:10px;width:calc(100% - 40px);padding-top:5px"};function O(e){const{0:a,1:t}=(0,s.useState)(!1);let i;return i=-1===e.abstract.indexOf(" ",250)?(0,b.Y)("div",null,e.abstract):(0,b.Y)("div",null,a?e.abstract+" ":e.abstract.slice(0,e.abstract.indexOf(". ")+2),(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>t((e=>!e))},"[",a?"Collapse":"Expand","]")),(0,b.Y)("div",{css:(0,b.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:250px;}","","","")},(0,b.Y)("a",{href:e.pdf,target:"_blank"},(0,b.Y)("div",{css:(0,b.AH)("font-weight:600;line-height:20px;color:",d.A.light.blue7,";font-size:15px;transition-duration:0.15s;&:hover{color:",d.A.light.blue6,";}","","","")},e.title)),(0,b.Y)("div",{css:(0,b.AH)("margin-bottom:8px;color:",d.A.gray8,";line-height:20px;font-size:13px;","","","")},Object.keys(e.authors).map(((a,t)=>(0,b.Y)(s.Fragment,null,(0,b.Y)("span",null,a),(0,b.Y)("sup",null),t!==Object.keys(e.authors).length-1?", ":"")))),i,(0,b.Y)("div",{css:X},(0,b.Y)(T,{text:"PDF",url:e.pdf}),(0,b.Y)(T,{text:"Poster",url:e.poster})))}let N=[(0,b.Y)(O,{title:"Pathdreamer: A World Model for Indoor Navigation",abstract:"People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360 degree visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. In the downstream task of Vision-and-Language Navigation (VLN), planning ahead with Pathdreamer provides about half the benefit of looking ahead at unobserved parts of the environment.",authors:{"Jing Yu Koh":[0],"Honglak Lee":[1,2],"Yinfei Yang":[0],"Jason Baldridge":[0],"Peter Anderson":[0]},affiliations:["Google Research","LG AI Research","University of Michigan"],pdf:"/papers/Pathdreamer.pdf",poster:"/posters/Pathdreamer_poster.pdf"}),(0,b.Y)(O,{title:"A Neural-Symbolic Approach for Object Navigation",abstract:"Object navigation refers to the task of discovering and locating objects in an unknown environment. End-to-end deep learning methods struggle at this task due to sparse rewards. In this work, we propose a simple neural-symbolic approach for object navigation in the AI2-THOR environment. Our method takes raw RGB images as input and uses a spatial memory graph as memory to store object and location information. The architecture consists of both a convolutional neural network for object detection and a spatial graph to represent the environment. By having a discrete graph representation of the environment, the agent can directly use search or planning algorithms as high-level reasoning engines. Model performance is evaluated on both task completion rate and steps required to reach target objects. Empirical results demonstrate that our approach can achieve performance close to the optimal. Our work builds a foundation for a neural-symbolic approach that can reason via unstructured visual cues.",authors:{"Xiaotian Liu":[0],"Christian Muise":[0]},affiliations:["Queen's University"],pdf:"/papers/A-Neural-Symbolic-Approach-for-Object-Navigation.pdf",poster:"/posters/Neural-Symbolic-Approach-for-Object-Navigation.pdf"}),(0,b.Y)(O,{title:"LegoTron: An Environment for Interactive Structural Understanding",abstract:"Visual reasoning about geometric structures with detailed spatial relationships is a fundamental component of human intelligence. As children, we learn how to reason about this structure not only from observation, but also by interacting with the world around us -- by taking things apart and putting them back together again. We introduce a new learning environment designed to explore the interplay between interactive reasoning, scene understanding and construction by mining a previously untapped high-quality data source: fan-made Lego creations that have been uploaded to the internet. To make use of this data we have built LegoTron, a fully interactive 3D environment that allows a learning agent to assemble, disassemble and manipulate these models. Our goal is to provide an interactive playground for agents to explore and manipulate complex scenes and recover their underlying structure.",authors:{"Aaron T Walsman":[0],"Muru Zhang":[0],"Adam Fishman":[0],"Karthik Desingh":[0],"Dieter Fox":[0,1],"Ali Farhadi":[0,2]},affiliations:["University of Washington, Seattle","NVIDIA","Apple"],pdf:"/papers/LegoTron.pdf",poster:"/posters/LegoTron.pdf"}),(0,b.Y)(O,{title:"Success-Aware Visual Navigation Agent",abstract:"This work presents a method to improve the efficiency and robustness of the previous model-free Reinforcement Learning (RL) algorithms for the task of object-target visual navigation. Despite achieving the state-of-the-art results, one of the major drawbacks of those approaches is the lack of a forward model that informs the agent about the potential consequences if its actions, e.g. being model-free. In this work we take a step towards augmenting the model-free methods with a forward model that is trained along with the policy, using a replay buffer, and can predict a successful future state of an episode in a challenging 3D navigation environment. We develop a module that can predict a representation of a future state, from the beginning of a navigation episode, if the episode were to be successful; we call this ForeSIM module. ForeSIM is trained to imagine a future latent state that leads to success. Therefore, during navigation, the policy is able to take better actions leading to two main advantages: first, in the absence of an object detector, ForeSIM leads mainly to a more robust policy, e.g. about 5% absolute improvement on success rate; second, when combined with an off-the-shelf object detector to help better distinguish the target object, ForeSIM leads to about 3% absolute improvement on success rate and about 2% absolute improvement on Success weighted by inverse Path Length (SPL), e.g. higher efficiency.",authors:{"Mahdi Kazemi Moghaddam":[0],"Ehsan M Abbasnejad":[0],"Qi Wu":[0],"Javen Qinfeng Shi":[0],"Anton van den Hengel":[0]},affiliations:["University of Adelaide"],pdf:"/papers/Success-Aware-Visual-Navigation-Agent.pdf",poster:"/posters/Success-Aware-Visual-Navigation-Agent.pdf"}),(0,b.Y)(O,{title:"Learning to Explore, Navigate and Interact for Visual Room Rearrangement",abstract:"Intelligent agents for visual room rearrangement aim to reach a goal room configuration from a cluttered room configuration via a sequence of interactions. For successful visual room rearrangement, the agents need to learn to explore, navigate and interact within the surrounding environments. Contemporary methods for visual room rearrangement display unsatisfactory performance even with state-of-the-art techniques for embodied AI. One of the causes for the low performance arises from the expensive cost of learning in an end-to-end manner. To overcome the limitation, we design a three-phased modular architecture (TMA) for visual room rearrangement. TMA performs visual room rearrangement in three phases: the exploration phase, the inspection phase, and the rearrangement phase. The proposed TMA maximizes the performance by placing the learning modules along with hand-crafted feature engineering modules—retaining the advantage of learning while reducing the cost of learning.",authors:{"Ue-Hwan Kim":[0],"Youngho Kim":[0],"Jin-Man Park":[0],"Hwansoo Choi":[0],"Jong-Hwan Kim":[0]},affiliations:["KAIST"],pdf:"/papers/Learning-to-Explore-Navigate-and-Interact-for-Visual-Room-Rearrangement.pdf",poster:"/posters/Learning-to-Explore.pdf"}),(0,b.Y)(O,{title:"Massively Parallel Robot Simulations with the HBP Neurorobotics Platform",abstract:"The success of deep learning in robotics hinges on the availability of physically accurate virtual training environments and simulation tools that accelerate learning by scaling to many parallel instances. However, most current AI frameworks do not integrate easily with common software stacks from robotics, while fully-fledged robot simulators lack capabilities for parallelization. In this paper, we introduce an extension for the Neurorobotics Platform of the Human Brain Project (HBP) that offers the full feature set of a robot simulation and at the same time is arbitrarily scalable for massively parallel robotics experiments.",authors:{"Florian Walter":[0],"Mahmoud Akl":[0],"Fabrice O. Morin":[0],"Alois Knoll":[0]},affiliations:["Technical University of Munich"],pdf:"/papers/Massively-Parallel-NRP-Simulations.pdf",poster:"/posters/Massively-Parallel-NRP-Simulations.pdf"}),(0,b.Y)(O,{title:"BEyond observation: an approach for ObjectNav",abstract:"With the rise of automation, unmanned vehicles became a hot topic both as commercial products and as a scientific research topic. It composes a multi-disciplinary field of robotics that encompasses embedded systems, control theory, path planning, Simultaneous Localization and Mapping (SLAM), scene reconstruction, and pattern recognition. In this work, we present our exploratory research of how sensor data fusion and state-of-the-art machine learning algorithms can perform the Embodied Artificial Intelligence (E-AI) task called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation (ObjectNav) consists of autonomous navigation using egocentric visual observations to reach an object belonging to the target semantic class without prior knowledge of the environment. Our method reached second place on the Habitat Challenge 2021 ObjectNav on the Minival phase.",authors:{"Daniel V Ruiz":[0],"Eduardo Todt":[0]},affiliations:["UFPR"],pdf:"/papers/BEyond-observation-an-approach-for-ObjectNav.pdf",poster:"/posters/BEyond.pdf"}),(0,b.Y)(O,{title:"PiCoEDL: Discovery and Learning of Minecraft Navigation Goals from Pixels and Coordinates",abstract:"Defining a reward function in Reinforcement Learning (RL) is not always possible or very costly. For this reason, there is a great interest in training agents in a task-agnostic manner making use of intrinsic motivations and unsupervised techniques. Due to the complexity to learn useful behaviours in pixel-based domains, the results obtained in RL are still far from the remarkable results obtained in domains such as computer vision and natural language processing. We hypothesize that RL agents will also benefit from unsupervised pre-trainings with no extrinsic rewards, analogously to how humans mostly learn, especially in the early stages of life. Our main contribution is the deployment of the Explore, Discover and Learn (EDL) paradigm for unsupervised learning to the pixel space. In particular, our work focuses on the MineRL environment, where the observation of the agent is represented by: (a) its spatial coordinates in the Minecraft virtual world, and (b) an image from an egocentric viewpoint. Following the idea of empowerment, our goal is to learn latent-conditioned policies by maximizing the mutual information between states and some latent variables, instead of sequences of actions. This allows the agent to influence the environment while discovering available skills.",authors:{"Juan José Nieto":[0],"Roger Creus Castanyer":[0],"Xavier Giró-i-Nieto":[0]},affiliations:["Universitat Politecnica de Catalunya"],pdf:"/papers/PiCoEDL.pdf",poster:"/posters/PiCoEDL.pdf"}),(0,b.Y)(O,{title:"Agent with the Big Picture: Perceiving Surroundings for Interactive Instruction Following",abstract:"Performing simple household tasks based on language directives is very natural to humans, yet it remains an open challenge for an AI agent. The 'interactive instruction following' task attempts to make progress towards building an agent that can jointly navigate, interact, and reason in the environment at every step. Here, we propose to exploit surrounding views by perceiving observation from navigable directions for effective task completion with ample information, In addition, to address the multifaceted problem, we propose a model that factorizes the task into interactive perception and action policy streams with enhanced components. We empirically validate that our model outperforms prior arts by significant margins on the ALFRED benchmark with improved generalization.",authors:{"Byeonghwi Kim":[0],"Suvaansh Bhambri":[0],"Kunal Pratap Singh":[1],"Roozbeh Mottaghi":[1,2],"Jonghyun Choi":[0]},affiliations:["GIST","Allen Institute for AI","University of Washington"],pdf:"/papers/Agent-with-the-Big-Picture.pdf",poster:"/posters/Agent-with-the-Big-Picture.pdf"}),(0,b.Y)(O,{title:"PixelEDL: Unsupervised Skill Discovery and Learning from Pixels",abstract:"We tackle embodied visual navigation in a task-agnostic set-up by putting the focus on the unsupervised discovery of skills (or options) that provide a good coverage of states. Our approach intersects with empowerment: we address the reward-free skill discovery and learning tasks to discover “what” can be done in an environment and “how”. For this reason, we adopt the existing Explore, Discover and Learn (EDL) paradigm, tested only in toy example mazes, and extend it to pixel-based state representations available for embodied AI agents.",authors:{"Roger Creus Castanyer":[0],"Juan José Nieto":[0],"Xavier Giro-i-Nieto":[0]},affiliations:["Universitat Politècnica de Catalunya"],pdf:"/papers/PixelEDL.pdf",poster:"/posters/PixelEDL.pdf"}),(0,b.Y)(O,{title:"URoboSim: A Simulation-Based Predictive Modelling Engine for Cognition-Enabled Robot Manipulation",abstract:"In a nutshell robot simulators are fully developed software systems that provide simulations as a substitute for real-world activity. They are primarily used for training modules of robot control programs, which are, after completing the learning process, deployed in real-world robots. In contrast, simulation in (artificial) cognitive systems is a core cognitive capability, which is assumed to provide a “small-scale model of external reality and of its own possible actions within its head, it is able to try out various alternatives, conclude which is the best of them, react to future situations before they arise, utilise the knowledge of past events in dealing with the present and future, and in every way to react in a much fuller, safer, and more competent manner to the emergencies which face it.” [8] This means that simulation can be considered as an embodied, online predictive modelling engine that enables robots to contextualize vague task requests such as “bring me the milk” into a concrete body motion that achieves the implicit goal and avoids unwanted side effects. In this setting a robot can run small-scale simulation and rendering processes for different reasoning tasks all the time and can continually compare simulation results with reality — it is a promising Sim2Real2Sim setup that has the potential to create much more powerful robot simulation engines. We introduce URoboSim, a robot simulation framework that is currently designed and developed with this vision in mind.",authors:{"Michael Neumann":[0],"Michael Beetz":[0],"Andrei Haidu":[0]},affiliations:["University Bremen"],pdf:"/papers/URoboSim.pdf",poster:"/posters/URoboSim.pdf"}),(0,b.Y)(O,{title:"RobustNav: Towards Benchmarking Robustness in Embodied Navigation",abstract:"As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual – affecting RGB inputs – and dynamics – affecting transition dynamics – corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean “non-corrupt” settings, warranting more research in this direction.",authors:{"Prithvijit Chattopadhyay":[0],"Judy Hoffman":[0],"Roozbeh Mottaghi":[1,2],"Aniruddha Kembhavi":[1,2]},affiliations:["Georgia Tech","Allen Institute for AI","University of Washington"],pdf:"/papers/RobustNav.pdf",poster:"/posters/RobustNav.pdf"}),(0,b.Y)(O,{title:"HexaJungle: a MARL Simulator to Study the Emergence of Language",abstract:"Multi-agent reinforcement learning in mixed-motive settings allows for the study of complex dynamics of agent interactions. Embodied agents in partially observable environments with the ability to communicate can share information, agree on strategies, or even lie to each other.In order to study this, we propose a simple environment where we can impose varying levels of cooperation, communication and competition as pre-requisites to reach an optimal outcome. Welcome to the jungle.",authors:{"Kiran Ikram":[0],"Esther Mondragon":[0],"Eduardo Alonso":[0],"Michaël Garcia Ortiz":[0]},affiliations:["City University Artificial Intelligence Lab"],pdf:"/papers/HexaJungle.pdf",poster:"/posters/HexaJungle.pdf"}),(0,b.Y)(O,{title:"Modular Framework for Visuomotor Language Grounding",abstract:"Natural language instruction following tasks serve as a valuable test-bed for grounded language and robotics research. However, data collection for these tasks is expensive and end-to-end approaches suffer from data inefficiency. We propose the structuring of language, acting, and visual tasks into separate modules that can be trained independently. Using a Language, Action, and Vision (LAV) framework removes the dependence of action and vision modules on instruction following datasets, making them more efficient to train. We also present a preliminary evaluation of LAV on the ALFRED task for visual and interactive instruction following.",authors:{"Kolby T Nottingham":[0],"Litian Liang":[0],"Daeyun Shin":[0],"Charless Fowlkes":[0],"Roy Fox":[0],"Sameer Singh":[0]},affiliations:["University of California Irvine"],pdf:"/papers/Modular-Framework-for-Visuomotor-Language-Grounding.pdf",poster:"/posters/LAV.pdf"}),(0,b.Y)(O,{title:"PGDrive: Procedural Generation of Driving Environments for Generalization",abstract:"To better evaluate and improve the generalization of end-to-end driving, we introduce an open-ended and highly configurable driving simulator called PGDrive, following a key feature of procedural generation. We validate that training with the increasing number of procedurally generated scenes significantly improves the generalization of the agent across scenarios of different traffic densities and road networks. Many applications such as multi-agent traffic simulation and safe driving benchmark can be further built upon the simulator.",authors:{"Quanyi Li":[0],"Zhenghao Peng":[0],"Qihang Zhang":[1],"Chunxiao Liu":[2],"Bolei Zhou":[0]},affiliations:["Chinese University of Hong Kong","Zhejiang University","Sensetime"],pdf:"/papers/PGDrive.pdf",poster:"/posters/pgdrive.pdf"})];const F=e=>(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")},e.time),G=function(e){var a,t=e.length;for(;0!==t;)a=Math.floor(Math.random()*t),t--,[e[t],e[a]]=[e[a],e[t]];return e}((0,o.A)(Array(N.length).keys()));var V={name:"5ou1pc",styles:"width:15px;margin-right:5px"},C={name:"1guecoj",styles:"display:inline-block;border-radius:0px 10px 0px 10px;padding-left:10px;padding-right:10px;margin-top:3px;padding-top:3px;padding-bottom:4px;background-color:#4a154b;transition-duration:0.15s;color:white;&:hover{cursor:pointer;filter:contrast(1.25);}>span,>img{vertical-align:middle;}"};function H(){return(0,b.Y)("div",null,(0,b.Y)("a",{href:"//join.slack.com/t/embodied-aiworkshop/shared_invite/zt-s6amdv5c-gBZQZ7YSktrD_tMhQDjDfg",target:"_blank"},(0,b.Y)("div",{css:C},(0,b.Y)("img",{src:f.A,css:V})," ",(0,b.Y)("span",null,"Ask questions on ",(0,b.Y)("b",null,"Slack")))))}var W={name:"y9fbzk",styles:"background-color:white;color:black;padding:5px;padding-top:6px;padding-bottom:3px;padding-left:5px;margin-top:12px;border-radius:10px 0px 10px 0px"},j={name:"1saok50",styles:"display:inline-block;vertical-align:middle"},E={name:"g8ulw8",styles:"height:20px;vertical-align:middle;margin-right:7px"};function U(){return(0,b.Y)("a",{href:"//join.slack.com/t/embodied-aiworkshop/shared_invite/zt-s6amdv5c-gBZQZ7YSktrD_tMhQDjDfg",target:"_blank"},(0,b.Y)("div",{css:(0,b.AH)("background-color:#4a154b;color:white;padding:15px 15px;border-radius:10px 0px 10px 0px;transition-duration:0.15s;&:hover{cursor:pointer;filter:contrast(1.25);box-shadow:0px 0px 15px 0px ",d.A.gray6,";}","","","")},(0,b.Y)("img",{src:f.A,css:E}),(0,b.Y)("div",{css:j},"Ask Questions on ",(0,b.Y)("b",null,"Slack")),(0,b.Y)("div",{css:W},"Questions can be asked"," ",(0,b.Y)("b",null,"anonymously"),".")))}var Q={name:"rzxmlp",styles:"display:grid;grid-gap:2%;grid-row-gap:20px;grid-template-columns:49% 49%;@media (max-width: 600px){grid-template-columns:100%;}"},_={name:"jdiuzp",styles:"margin-top:25px;margin-bottom:50px"},J={name:"n2q596",styles:">p{margin-top:5px;text-indent:-10px;margin-left:10px;&:nth-of-type(1){margin-top:20px;}}"},K={name:"n2q596",styles:">p{margin-top:5px;text-indent:-10px;margin-left:10px;&:nth-of-type(1){margin-top:20px;}}"},B={name:"n2q596",styles:">p{margin-top:5px;text-indent:-10px;margin-left:10px;&:nth-of-type(1){margin-top:20px;}}"},q={name:"1qs5g6e",styles:"margin-left:0px;margin-top:20px"};function Z(e){let{data:a}=e;const{0:t,1:o}=(0,s.useState)(M());(0,s.useEffect)((()=>{const e=()=>o(M());return window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)}));const p=[{challenge:k["AI2-THOR ObjectNav"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=bUW8oGKqtY8&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"ai2thor2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=EH94fGEjj1I&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"ict_robothor",data:a,width:"200px",rank:"1st Place"})),key:"ai2thor-objectnav",task:"ObjectNav",interactiveActions:"",simulationPlatform:"AI2-THOR",sceneDataset:"RoboTHOR",actionSpace:"Discrete",observations:"RGB-D",stochasticAcuation:"✓"},{challenge:k["AI2-THOR Rearrangement"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=bUW8oGKqtY8&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"ai2thor2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"ai2thor-rearrangement",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:k.ALFRED,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=-YmHT2fSQDo&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"alfred2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"alfred",task:"Vision-and-Language Interaction",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB",stochasticAcuation:""},{challenge:k.Habitat,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=SAoN2giK6Gk&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"habitat2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=z7HflwSv3GM&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"redRabbit_habitat",data:a,rank:"1st Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=40cbSZefjjY&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"habitatOnWeb_habitat",data:a,rank:"2nd Place"})),key:"habitat-objectNav",task:"ObjectNav",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:k.Habitat,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=SAoN2giK6Gk&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"habitat2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=I-4s2keQ1Ig&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"inspir_habitat",data:a,rank:"1st Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=z1lYiPfEAOQ&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"ucu_habitat",data:a,rank:"2nd Place"})),key:"habitat-pointnav",task:"PointNav v2",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Gibson",actionSpace:"Discrete",observations:"Noisy RGB-D",stochasticAcuation:"✓"},{challenge:k.iGibson,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=1uSsds7HSrQ&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"igibson2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=kC9wdC3abDo&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"gatech_igibson",data:a,display:"block",rank:"1st Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=x5ewIkkgYuQ&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"lpais_igibson",data:a,display:"block",rank:"2nd Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=gK4ek_tvCJo&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"nicsefc_igibson",data:a,display:"block",rank:"4th Place"})),key:"igibson-in",task:"Interactive Navigation",interactiveActions:"✓",simulationPlatform:"iGibson",sceneDataset:"iGibson",actionSpace:"Continuous",observations:"RGB-D",stochasticAcuation:"✓"},{challenge:k.iGibson,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=1uSsds7HSrQ&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"igibson2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=gK4ek_tvCJo&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"nicsefc_igibson",data:a,rank:"1st Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=x5ewIkkgYuQ&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"lpais_igibson",data:a,rank:"3rd Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=c2TRfio7J-M&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"lpacsi_igibson",data:a,rank:"4th Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=kC9wdC3abDo&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"gatech_igibson",data:a,rank:"5th Place"})),key:"igibson-social-navigation",task:"Social Navigation",interactiveActions:"✓",simulationPlatform:"iGibson",sceneDataset:"iGibson",actionSpace:"Continuous",observations:"RGB-D",stochasticAcuation:"✓"},{challenge:k.MultiOn,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=ghX5UDWD1HU&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"multion2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null,(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=boDaAORoKho&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"lyon_multion",data:a,display:"block",rank:"1st Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=XgT2w6rUwjM&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"sgolam_multion",data:a,display:"block",rank:"2nd Place"}),(0,b.Y)(Y,{url:"//www.youtube.com/watch?v=AIbVRo4xIh8&list=PL4XI7L9Xv5fVnzoKzSL0GOu2l2fIAJA7O",imageQuery:"vimp_multion",data:a,display:"block",rank:"3rd Place"})),key:"multion",task:"Multi-Object Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:k["Robotic Vision Scene Understanding"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=uDhIEw9TA80&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"rvsu2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"rvsu",task:"Rearrangement (SCD)",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"✓"},{challenge:k["Robotic Vision Scene Understanding"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=uDhIEw9TA80&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"rvsu2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"rvsu-2",task:"Semantic SLAM",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"Partially"},{challenge:k["RxR-Habitat"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=YGwHGgD-9gQ&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"rxr2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"rxr",task:"Vision-and-Language Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D",actionSpace:"Discrete",stochasticAcuation:""},{challenge:k.SoundSpaces,video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=ANmhSo6gXNg&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"soundspaces2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"soundspaces",task:"Audio Visual Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D, Audio Waveform",actionSpace:"Discrete",stochasticAcuation:""},{challenge:k["TDW-Transport"],video:(0,b.Y)(y,{url:"//www.youtube.com/watch?v=ffh7zxWAkFw&list=PL4XI7L9Xv5fVzPkYPxASt64LhfNM8MAlP",imageQuery:"tdw2021",data:a}),winnerSpotlight:(0,b.Y)(s.Fragment,null),key:"tdw",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"TDW",sceneDataset:"TDW",observations:"RGB-D, Metadata",actionSpace:"Discrete",stochasticAcuation:"✓"}],h=g().tz("2021-05-15 04:59","America/Los_Angeles"),f=g()(),A=g().duration(h.diff(f));Math.ceil(A.asHours()%24),Math.floor(A.asDays());return(0,b.Y)(c.A,{headerGradient:"linear-gradient(to bottom, #ebdfa5, #49c3cd)",imageContent:{css:(0,b.AH)("width:120%;background-repeat:no-repeat;padding-top:50.25%;margin-left:-6%;margin-top:50px;margin-bottom:-15px;background-image:url(",m.A,");","","","")},conference:"CVPR 2021",rightSide:(0,b.Y)(u.NT,{conference:"CVPR 2021",challengeData:Object.values(k)})},(0,b.Y)(l.wn,{title:"Overview"},(0,b.Y)("p",null,"Within the last decade, advances in deep learning, coupled with the creation of large, freely available datasets (e.g., ImageNet), have resulted in remarkable progress in the computer vision, NLP, and broader AI communities. This progress has enabled models to begin to obtain superhuman performance on a wide variety of passive tasks. However, this progress has also enabled a paradigm shift that a growing collection of researchers take aim at: the creation of an embodied agent (e.g., a robot) which learns, through interaction and exploration, to creatively solve challenging tasks within its environment."),(0,b.Y)("p",null,"The goal of this workshop is to bring together researchers from the fields of computer vision, language, graphics, and robotics to share and discuss the current state of intelligent agents that can:"),(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"See"),": perceive their environment through vision or other senses."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Talk"),": hold a natural language dialog grounded in their environment."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Listen"),": understand and react to audio input anywhere in a scene."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Act"),": navigate and interact with their environment to accomplish goals."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Reason"),": consider and plan for the long-term consequences of their actions.")),(0,b.Y)("p",null,"The Embodied AI 2021 workshop will be held virtually in conjunction with CVPR 2021. It will feature a host of invited talks covering a variety of topics in Embodied AI, many exciting challenges, a poster session, and panel discussions."),(0,b.Y)(z,{actionIdentifier:"1FAIpQLSeIZrn-tk7Oain2R8gc_Q0HzLMLQ9XXwqu3KecK_E5kALpiug",entryNumber:1834823104})),(0,b.Y)(l.wn,{title:"Timeline"},(0,b.Y)(r.A,{progressDot:!0,current:0,direction:"vertical"},(0,b.Y)(w,{title:"CVPR Workshop",description:(0,b.Y)(s.Fragment,null,"June 20, 2021."," ",(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")},"Tentative Schedule:"),(0,b.Y)("div",{css:q},(0,b.Y)(n.A,null,(0,b.Y)(n.A.Item,null,"Livestream",(0,b.Y)("br",null),(0,b.Y)(F,{time:"6:30 AM - 6:00 PM PST"})),(0,b.Y)(n.A.Item,null,"Speaker Panel",(0,b.Y)("br",null),(0,b.Y)(F,{time:"11:00 AM PST"}),(0,b.Y)(H,null)),(0,b.Y)(n.A.Item,null,"Lunch",(0,b.Y)("br",null),(0,b.Y)(F,{time:"12:00 AM PST"})),(0,b.Y)(n.A.Item,null,"Poster Session",(0,b.Y)("br",null),(0,b.Y)(F,{time:"1:00 PM PST"}),(0,b.Y)("div",null,(0,b.Y)("div",{css:(0,b.AH)("opacity:0.2;display:inline-block;border-radius:0px 10px 0px 10px;padding-left:10px;padding-right:10px;margin-top:3px;padding-top:3px;padding-bottom:4px;background-color:",d.A.dark.blue6,";transition-duration:0.15s;color:white;&:hover{cursor:not-allowed;filter:contrast(1.25);}>span,>img{vertical-align:middle;}","","","")},(0,b.Y)("span",null,"Join on ",(0,b.Y)("b",null,"gather.town"))))),(0,b.Y)(n.A.Item,null,"Navigation Challenge Results",(0,b.Y)("br",null),(0,b.Y)(F,{time:"2:00 PM PST"})),(0,b.Y)(n.A.Item,null,"Navigation Panel",(0,b.Y)("br",null),(0,b.Y)(F,{time:"3:00 PM PST"}),(0,b.Y)(H,null)),(0,b.Y)(n.A.Item,null,"Interaction Challenge Results",(0,b.Y)("br",null),(0,b.Y)(F,{time:"4:00 PM PST"})),(0,b.Y)(n.A.Item,null,"Interaction Panel",(0,b.Y)("br",null),(0,b.Y)(F,{time:"5:00 PM PST"}),(0,b.Y)(H,null)))))}),(0,b.Y)(w,{title:"Challenge Submission Deadlines",description:"May 2021. Check each challenge for the specific date."}),(0,b.Y)(w,{title:"Paper Submission Deadline",description:"May 14, 2021 (Anywhere on Earth)"}),(0,b.Y)(w,{title:"Workshop Announced",description:"Feb 17, 2021"}))),(0,b.Y)(l.wn,{title:"Panel Sessions"},(0,b.Y)(l.Wo,{title:"Speaker Panel"},(0,b.Y)(v.LiveSession,{fluidImage:a.speakerPanel.childImageSharp.fluid,videoURL:"//www.youtube.com/watch?v=-UcfQnTk8dU&list=PL4XI7L9Xv5fXNCizY4FUOT69pnG5c5KLy",rhs:(0,b.Y)(s.Fragment,null,(0,b.Y)(U,null),(0,b.Y)("div",{css:B},(0,b.Y)("p",null,(0,b.Y)("b",null,"Date.")," June 20th, 11 AM PST."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Panel.")," The panel consists of speakers at this workshop."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Moderator.")," Erik Wijmans."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Topics.")," The topics are based on questions, likely involving cognitive development in humans, progress in embodied AI tasks, sim-2-real transfer, robotics, embodied AI for all, and more!")))})),(0,b.Y)(l.Wo,{title:"Navigation Panel"},(0,b.Y)(v.LiveSession,{fluidImage:a.navigationPanel.childImageSharp.fluid,videoURL:"//www.youtube.com/watch?v=dOR-Q0XS6Xs&list=PL4XI7L9Xv5fXNCizY4FUOT69pnG5c5KLy",rhs:(0,b.Y)(s.Fragment,null,(0,b.Y)(U,null),(0,b.Y)("div",{css:K},(0,b.Y)("p",null,(0,b.Y)("b",null,"Date.")," June 20th, 3 PM PST."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Panel.")," The panel consists of challenge organizers who organized navigation tasks."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Moderator.")," Luca Weihs."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Topics."),' The topics are based on questions, likely involving navigation benchmarks and tasks, the "reality" gap, robotics, simulation platforms, and more!')))})),(0,b.Y)(l.Wo,{title:"Interaction Panel"},(0,b.Y)(v.LiveSession,{fluidImage:a.interactionPanel.childImageSharp.fluid,videoURL:"//www.youtube.com/watch?v=kQ4mxaGd21M&list=PL4XI7L9Xv5fXNCizY4FUOT69pnG5c5KLy",rhs:(0,b.Y)(s.Fragment,null,(0,b.Y)(U,null),(0,b.Y)("div",{css:J},(0,b.Y)("p",null,(0,b.Y)("b",null,"Date.")," June 20th, 5 PM PST."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Panel.")," The panel consists of challenge organizers who organized interaction tasks."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Moderator.")," Chengshu (Eric) Li."),(0,b.Y)("p",null,(0,b.Y)("b",null,"Topics."),' The topics are based on questions, likely involving interaction benchmarks and tasks, vision-and-language, rearrangement, leveraging audio, the "reality" gap, robotics, simulation platforms, and more!')))}))),(0,b.Y)(l.wn,{title:"Invited Speakers"},(0,b.Y)(l.Wo,{title:"Motivation for Embodied AI Research"},(0,b.Y)(v.Speaker,{organizations:["Stanford"],name:"Hyowon Gweon",fixedImg:a.hyowon.childImageSharp.fixed,url:"//www.youtube.com/watch?v=1S8lUbkuMnk&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"})),(0,b.Y)(l.Wo,{title:"Embodied Navigation"},(0,b.Y)(v.Speaker,{organizations:["Google"],name:"Peter Anderson",fixedImg:a.peter.childImageSharp.fixed,url:"//www.youtube.com/watch?v=r5RmmXeUAwE&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"}),(0,b.Y)(v.Speaker,{organizations:["Google"],name:"Aleksandra Faust",fixedImg:a.aleksandra.childImageSharp.fixed,url:"//www.youtube.com/watch?v=x0CXtjpsWCE&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"})),(0,b.Y)(l.Wo,{title:"Robotics"},(0,b.Y)(v.Speaker,{organizations:["UC Berkeley"],name:"Anca Dragan",fixedImg:a.anca.childImageSharp.fixed,url:"//www.youtube.com/watch?v=G-qxzerBq8I&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"}),(0,b.Y)(v.Speaker,{organizations:["Stanford","Google"],name:"Chelsea Finn",fixedImg:a.chelsea.childImageSharp.fixed,url:"//www.youtube.com/watch?v=6IGdWmvcwb4&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"}),(0,b.Y)(v.Speaker,{organizations:["Facebook AI Research"],name:"Akshara Rai",fixedImg:a.akshara.childImageSharp.fixed,url:"//www.youtube.com/watch?v=Z3RMJA1Nopw&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"})),(0,b.Y)(l.Wo,{title:"Sim-2-Real Transfer"},(0,b.Y)(v.Speaker,{organizations:["University of Toronto","NVIDIA"],name:"Sanja Fidler",fixedImg:a.sanja.childImageSharp.fixed,url:"//www.youtube.com/watch?v=cgAatW67U4M&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"}),(0,b.Y)(v.Speaker,{organizations:["DeepMind"],name:"Konstantinos Bousmalis",fixedImg:a.konstantinos.childImageSharp.fixed,url:"//www.youtube.com/watch?v=jqxY7tqc6-Y&list=PL4XI7L9Xv5fWVW72Dmoqkc3lJUnF67jvF"}))),(0,b.Y)(l.wn,{title:"Challenges"},(0,b.Y)("p",null,"The Embodied AI 2021 workshop is hosting many exciting challenges covering a wide range of topics such as rearrangement, visual navigation, vision-and-language, and audio-visual navigation. More details regarding data, submission instructions, and timelines can be found on the individual challenge websites."),(0,b.Y)("p",null,"Challenge winners will be given the opportunity to present a talk at the workshop. Since many challenges can be grouped into similar tasks, we encourage participants to submit models to more than 1 challenge. The table below describes, compares, and links each challenge."),(0,b.Y)(i.A,{scroll:{x:"2250px"},css:_,sticky:!0,columns:[{title:(0,b.Y)(s.Fragment,null,"Challenge"),dataIndex:"challenge",key:"challenge",fixed:t>650?"left":""},{title:(0,b.Y)(s.Fragment,null,"Task"),dataIndex:"task",key:"task",sorter:(e,a)=>e.task.localeCompare(a.task),sortDirections:["ascend","descend"],fixed:t>650?"left":""},{title:(0,b.Y)(s.Fragment,null,"Video"),dataIndex:"video",key:"video",width:300},{title:(0,b.Y)(s.Fragment,null,"Spotlight"),dataIndex:"winnerSpotlight",key:"winnerSpotlight",width:400},{title:(0,b.Y)(s.Fragment,null,"Interactive Actions?"),dataIndex:"interactiveActions",key:"interactiveActions",sorter:(e,a)=>e.interactiveActions.localeCompare(a.interactiveActions),sortDirections:["descend","ascend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Simulation Platform"),dataIndex:"simulationPlatform",key:"simulationPlatform",sorter:(e,a)=>e.simulationPlatform.localeCompare(a.simulationPlatform),sortDirections:["ascend","descend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Scene Dataset"),dataIndex:"sceneDataset",key:"sceneDataset",sorter:(e,a)=>e.sceneDataset.localeCompare(a.sceneDataset),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"Observations"),key:"observations",dataIndex:"observations",sorter:(e,a)=>e.observations.localeCompare(a.observations),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"Stochastic Acuation?"),key:"stochasticAcuation",dataIndex:"stochasticAcuation",sorter:function(e,a){let t="✓"===e.stochasticAcuation?"Z":e.stochasticAcuation,i="✓"===a.stochasticAcuation?"Z":a.stochasticAcuation;return t.localeCompare(i)},sortDirections:["descend","ascend"],width:205},{title:(0,b.Y)(s.Fragment,null,"Action Space"),key:"actionSpace",dataIndex:"actionSpace",sorter:(e,a)=>e.actionSpace.localeCompare(a.actionSpace),sortDirections:["ascend","descend"]}],dataSource:p,pagination:!1})),(0,b.Y)(l.wn,{title:"Call for Papers"},(0,b.Y)("p",null,"We invite high-quality 2-page extended abstracts in relevant areas, such as:",(0,b.Y)("ul",null,(0,b.Y)("li",null,"Simulation Environments"),(0,b.Y)("li",null,"Visual Navigation"),(0,b.Y)("li",null,"Rearrangement"),(0,b.Y)("li",null,"Embodied Question Answering"),(0,b.Y)("li",null,"Simulation-to-Real Transfer"),(0,b.Y)("li",null,"Embodied Vision & Language")),"Accepted papers will be presented as posters. These papers will be made publicly available in a non-archival format, allowing future submission to archival journals or conferences."),(0,b.Y)(l.Wo,{title:"Submission"},(0,b.Y)("p",null,"The submission deadline is May 14th (",(0,b.Y)("a",{href:"//time.is/Anywhere_on_Earth"},"Anywhere on Earth"),"). Papers should be no longer than 2 pages (excluding references) and styled in the"," ",(0,b.Y)("a",{href:"http://cvpr2021.thecvf.com/node/33",target:"_blank"},"CVPR format"),". Paper submissions are now closed.")),(0,b.Y)(l.Wo,{title:"Accepted Papers"},(0,b.Y)("p",null,(0,b.Y)("b",null,"Note.")," The order of the papers is randomized each time the page is refreshed."),(0,b.Y)("div",{css:Q},G.map((e=>N[e]))))),(0,b.Y)(l.wn,{title:"Organizers"},"The Embodied AI 2021 workshop is a joint effort by a large set of researchers from a variety of organizations. They are listed below in alphabetical order.",(0,b.Y)(l.Wo,{title:"Organizing Committee"},(0,b.Y)(v.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2021.organizers.filter((e=>!0===e.oc)),data:a})),(0,b.Y)(l.Wo,{title:"Challenge Organizers"},(0,b.Y)(v.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2021.organizers.filter((e=>!0===e.challenge)),data:a})),(0,b.Y)(l.Wo,{title:"Scientific Advisory Board"},(0,b.Y)(v.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2021.organizers.filter((e=>!0===e.sab)),data:a}))))}}}]);
//# sourceMappingURL=e2c2d1f6-50178b2f5ce3c0622c76.js.map