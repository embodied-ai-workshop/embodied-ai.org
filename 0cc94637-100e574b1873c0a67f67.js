"use strict";(self.webpackChunkembodied_ai_workshop=self.webpackChunkembodied_ai_workshop||[]).push([[52],{28:function(e,a,t){t.r(a),t.d(a,{default:function(){return V}});t(3960);var n=t(1381),i=(t(1651),t(9304)),o=t(436),r=(t(9412),t(7262)),s=t(6540),l=t(6568),c=t(3020),d=t(2316),h=t(2158),u=t(4743),g=t.n(u),m=(t(4333),t(853)),p=t(7850),f=t(1013);const{Step:b}=r.A;const v={"AI2-THOR Rearrangement":(0,f.Y)("a",{href:"//ai2thor.allenai.org/rearrangement",target:"_blank"},"AI2-THOR Rearrangement"),ALFRED:(0,f.Y)("a",{href:"//askforalfred.com/EAI22",target:"_blank"},"ALFRED"),Habitat:(0,f.Y)("a",{href:"//aihabitat.org/challenge/2022",target:"_blank"},"Habitat"),iGibson:(0,f.Y)("a",{href:"http://svl.stanford.edu/igibson/challenge.html",target:"_blank"},"iGibson"),MultiOn:(0,f.Y)("a",{href:"http://multion-challenge.cs.sfu.ca",target:"_blank"},"MultiON"),"Robotic Vision Scene Understanding":(0,f.Y)("a",{href:"http://cvpr2022.roboticvisionchallenge.org/",target:"_blank"},"Robotic Vision Scene Understanding"),"RxR-Habitat":(0,f.Y)("a",{href:"//ai.google.com/research/rxr/habitat",target:"_blank"},"RxR-Habitat"),SoundSpaces:(0,f.Y)("a",{href:"//soundspaces.org/challenge",target:"_blank"},"SoundSpaces"),"TDW-Transport":(0,f.Y)("a",{href:"http://tdw-transport.csail.mit.edu",target:"_blank"},"TDW-Transport"),TEACh:(0,f.Y)("a",{href:"//teachingalfred.github.io/EAI22",target:"_blank"},"TEACh")};var y={name:"14y6t8x",styles:"display:none!important"},w={name:"99y604",styles:"margin-top:5px;color:#8c8c8c"},k={name:"rowx5j",styles:"border-radius:5px;box-shadow:0px 0px 2px 0px #2b4acb;display:inline-block;margin:auto;*{padding-top:3px;padding-bottom:5px;}"},A={name:"nl3i1y",styles:"vertical-align:middle;display:inline-block;margin-top:6px;margin-left:5px"},Y={name:"13sa3rf",styles:'font-weight:bold;font-size:25px;color:"#2b4acb";vertical-align:middle;display:inline-block'},x={name:"pr10xp",styles:"margin-bottom:10px"},S={name:"uewfz3",styles:"text-align:center;margin-top:60px;margin-bottom:60px"};function I(e){const{0:a,1:t}=(0,s.useState)(!1),{0:n,1:i}=(0,s.useState)(!1),{0:o,1:r}=(0,s.useState)(""),l=/^(([^<>()[\]\\.,;:\s@"]+(\.[^<>()[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/.test(String(o).toLowerCase());return(0,f.Y)("div",{css:S},(0,f.Y)("form",{encType:"text/plain",action:l?"https://docs.google.com/forms/d/e/"+e.actionIdentifier+"/formResponse?usp=pp_url&entry."+e.entryNumber+"="+o:"",target:"hidden_iframe"+e.actionIdentifier,onSubmit:()=>!!l&&t(!0),method:"post"},(0,f.Y)("div",{css:x},(0,f.Y)("div",{css:Y},"Sign Up for Updates"),(0,f.Y)("div",{css:A})),a?(0,f.Y)("div",null,"Thanks for signing up!"):(0,f.Y)(s.Fragment,null,(0,f.Y)("div",{css:k},(0,f.Y)("input",{type:"email",autoComplete:"off",placeholder:"email",name:"entry."+e.entryNumber,id:"entry."+e.entryNumber,onFocus:()=>i(!0),onBlur:()=>i(!1),onChange:e=>r(e.target.value),value:o,css:(0,f.AH)("background-color:transparent;transition-duration:0.3s;box-shadow:0px 0px 1px 2px ",n||l||""==o?"transparent":"#ff7875",";border:none;width:350px;@media (max-width: 500px){width:55vw;}border-radius:5px;padding-left:8px;","","","")}),(0,f.Y)("input",{type:l?"submit":"button",value:"Sign Up",onClick:()=>!!l,css:(0,f.AH)("background-color:transparent;border:none;font-weight:600;transition-duration:0.3s;color:",l?"#2b4acb":"#2b4acb88",";padding-top:3px;padding-right:12px;padding-left:10px;&:hover{cursor:",l?"pointer":"default",";}","","","")})),(0,f.Y)("div",{css:w},"You can unsubscribe at any time."))),(0,f.Y)("iframe",{name:"hidden_iframe"+e.actionIdentifier,id:"hidden_iframe"+e.actionIdentifier,css:y}))}function R(){if("undefined"==typeof window)return 800;const{innerWidth:e}=window;return e}var D={name:"nkt64x",styles:"margin-right:10px"};function C(e){return(0,f.Y)("a",{href:e.url,target:"_blank",css:D},(0,f.Y)("div",{css:(0,f.AH)("display:inline-block;border:1px solid ",d.A.gray5,";background-color:",d.A.gray2,";padding-left:7px;padding-right:7px;border-radius:5px;transition-duration:0.15s;>span{vertical-align:middle;}&:hover{background-color:",d.A.gray4,";border:1px solid ",d.A.gray6,";}","","","")},(0,f.Y)("span",{css:(0,f.AH)("margin-left:5px;color:",d.A.gray10,";","","","")},e.text)))}var L={name:"4ozr74",styles:"position:absolute;bottom:10px;width:calc(100% - 40px);padding-top:5px"};function T(e){const{0:a,1:t}=(0,s.useState)(!1);let n;return n=-1===e.abstract.indexOf(" ",250)?(0,f.Y)("div",null,e.abstract):(0,f.Y)("div",null,a?e.abstract+" ":e.abstract.slice(0,e.abstract.indexOf(". ")+2),(0,f.Y)("span",{css:(0,f.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>t((e=>!e))},"[",a?"Collapse":"Expand","]")),(0,f.Y)("div",{css:(0,f.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:250px;}","","","")},(0,f.Y)("a",{href:e.pdf,target:"_blank"},(0,f.Y)("div",{css:(0,f.AH)("font-weight:600;line-height:20px;color:",d.A.light.blue7,";font-size:15px;transition-duration:0.15s;&:hover{color:",d.A.light.blue6,";}","","","")},e.title)),(0,f.Y)("div",{css:(0,f.AH)("margin-bottom:8px;color:",d.A.gray8,";line-height:20px;font-size:13px;","","","")},Object.keys(e.authors).map(((a,t)=>(0,f.Y)(s.Fragment,null,(0,f.Y)("span",null,a),(0,f.Y)("sup",null),t!==Object.keys(e.authors).length-1?", ":"")))),n,(0,f.Y)("div",{css:L},(0,f.Y)(C,{text:"PDF",url:e.pdf}),e.poster?(0,f.Y)(C,{text:"Poster",url:e.poster}):(0,f.Y)(s.Fragment,null)))}let M=[(0,f.Y)(T,{title:"ABCDE: An Agent-Based Cognitive Development Environment",abstract:"Children’s cognitive abilities are sometimes cited as AI benchmarks. How can the most common 1,000 concepts (89% of everyday use) be learnt in a naturalistic children’s setting? Cognitive development in children is about quality, and new concepts can be conveyed via simple examples. Our approach of knowledge scaffolding uses simple objects and actions to convey concepts, like how children are taught. We introduce ABCDE, an interactive 3D environment modeled after a typical playroom for children. It comes with 300+ unique 3D object assets (mostly toys), and a large action space for child and parent agents to interact with objects and each other. ABCDE is the first environment aimed at mimicking a naturalistic setting for cognitive development in children; no other environment focuses on high-level concept learning through learner-teacher interactions. The simulator can be found at https://pypi.org/project/ABCDESim/1.0.0/",authors:{"Jieyi Ye":[],"Jiafei Duan":[],"Samson Yu":[],"Bihan Wen":[],"Cheston Tan":[]},affiliations:[],pdf:"/papers/2022/1.pdf"}),(0,f.Y)(T,{title:"IFOR: Iterative Flow Minimization for Robotic Object Rearrangement",abstract:"Accurate object rearrangement from vision is a crucial problem for a wide variety of real-world robotics applications in unstructured environments. We propose IFOR, Iterative Flow Minimization for Robotic Object Rearrangement, an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. First, we learn an optical flow model based on RAFT to estimate the relative transformation of the objects purely from synthetic data. This flow is then used in an iterative minimization algorithm to achieve accurate positioning of previously unseen objects. Crucially, we show that our method applies to cluttered scenes, and in the real world, while training only on synthetic data. Videos are available at https://imankgoyal.github.io/ifor.html.",authors:{"Ankit Goyal":[],"Arsalan Mousavian":[],"Chris Paxton":[],"Yu-Wei Chao":[],"Brian Okorn":[],"Jia Deng":[],"Dieter Fox":[]},affiliations:[],pdf:"/papers/2022/3.pdf"}),(0,f.Y)(T,{title:"Simple and Effective Synthesis of Indoor 3D Scenes",abstract:"We study the problem of synthesizing immersive 3D indoor scenes from one or more images. Our aim is to generate high-resolution images and videos from novel viewpoints, including those that extrapolate far beyond the input images while maintaining 3D consistency. Existing approaches are highly complex, with many separately trained stages and components. We propose a simple alternative: an image-to-image GAN that maps directly from reprojections of incomplete point clouds to full high-resolution RGB-D images. On the Matterport3D and RealEstate10K datasets, our approach significantly outperforms prior work when evaluated by humans, as well as on FID scores. Further, we show that our model is useful for generative data augmentation. A vision-and-language navigation (VLN) agent trained with trajectories spatially-perturbed by our model improves success rate by up to 1.5% over a state of the art model on the R2R benchmark. For more details, we refer readers to our full paper (https://arxiv.org/abs/2204.02960) and video results (https://youtu.be/lhwwlrRfFp0).",authors:{"Jing Yu Koh":[],"Harsh Agrawal":[],"Dhruv Batra":[],"Richard Tucker":[],"Austin Waters":[],"Honglak Lee":[],"Yinfei Yang":[],"Jason M Baldridge":[],"Peter Anderson":[]},affiliations:[],pdf:"/papers/2022/4.pdf"}),(0,f.Y)(T,{title:"Less is More: Generating Grounded Navigation Instructions from Landmarks",abstract:"We study the automatic generation of navigation instructions from 360-degree images captured on indoor routes. Existing generators suffer from poor visual grounding, causing them to rely on language priors and hallucinate objects. Our Marky-mT5 system addresses this by focusing on visual landmarks; it comprises a first stage landmark detector and a second stage generator--a multimodal, multilingual, multitask encoder-decoder. To train it, we bootstrap grounded landmark annotations on top of the Room-across-Room (RxR) dataset. Using text parsers, weak supervision from RxR's pose traces, and a multilingual image-text encoder trained on 1.8b images, we identify 971k English, Hindi and Telugu landmark descriptions and ground them to specific regions in panoramas. On Room-to-Room, human wayfinders obtain success rates (SR) of 71% following Marky-mT5's instructions, just shy of their 75% SR following human instructions---and well above SRs with other generators. Evaluations on RxR's longer, diverse paths obtain 61-64% SRs on three languages. Generating such high-quality navigation instructions in novel environments is a step towards conversational navigation tools and could facilitate larger-scale training of instruction-following agents. The full paper at CVPR 2022 is available at https://arxiv.org/abs/2111.12872.",authors:{"Su Wang":[],"Ceslee Montgomery":[],"Jordi Orbay":[],"Vighnesh N Birodkar":[],"Aleksandra Faust":[],"Izzeddin Gur":[],"Natasha Jaques":[],"Austin Waters":[],"Jason M Baldridge":[],"Peter Anderson":[]},affiliations:[],pdf:"/papers/2022/5.pdf"}),(0,f.Y)(T,{title:"VLMbench: A Benchmark for Vision-and-Language Manipulation",abstract:"One crucial ability of embodied agents is to finish tasks by following language instructions. Language can represent complicated tasks and distinguish their differences, and it is natural for humans to use language to command an embodied agent. In this work, we aim to fill the blank of the last mile of embodied agents---object manipulation by following human guidance, e.g., “move the red mug next to the box while keeping it upright.” To this end, we introduce an Automatic Manipulation Solver (AMSolver) and build a Vision-and-Language Manipulation benchmark (VLMbench), which contains various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.",authors:{"Kaizhi Zheng":[],"Xiaotong Chen":[],"Odest Chadwicke Jenkins":[],"Xin Eric Wang":[]},affiliations:[],pdf:"/papers/2022/6.pdf"}),(0,f.Y)(T,{title:"FedVLN: Privacy-preserving Federated Vision-and-Language Navigation",abstract:"Data privacy is a central problem for embodied agents that can perceive the environment, communicate with humans, and act in the real world. While helping humans complete tasks, the agent may observe and process sensitive information of users. In this work, we introduce privacy-preserving embodied agent learning for the task of Vision-and-Language Navigation (VLN), where an embodied agent navigates house environments by following natural language instructions. We propose a novel federated vision-and-language navigation (FedVLN) framework to protect data privacy during both training and pre-exploration, where we view each house environment as a local client. Experiment results show that, under our FedVLN framework, the decentralized VLN model achieve comparable results with centralized training while protecting seen environment privacy, and federated pre-exploration significantly outperforms other pre-exploration methods while preserving unseen environment privacy.",authors:{"Kaiwen Zhou":[],"Xin Eric Wang":[]},affiliations:[],pdf:"/papers/2022/7.pdf"}),(0,f.Y)(T,{title:"Towards Generalisable Audio Representations for Audio-Visual Navigation",abstract:"In audio-visual navigation (AVN), an intelligent agent needs to navigate to a constantly sound-making object in complex 3D environments based on its audio and visual perceptions. While existing methods attempt to improve the navigation performance with preciously designed path planning or intricate task settings, none has improved the model generalisation on unheard sounds with task settings unchanged. We thus propose a contrastive learning-based method to tackle this challenge by regularising the audio encoder, where the sound-agnostic goal-driven latent representations can be learnt from various audio signals of different classes. In addition, we consider two data augmentation strategies to enrich the training sounds. We demonstrate that our designs can be easily equipped to existing AVN frameworks to obtain an immediate performance gain (13.4%↑ in SPL on Replica and 12.2%↑ in SPL on MP3D). Our project is available at https://AV-GeN.github.io/.",authors:{"Shunqi Mao":[],"Chaoyi Zhang":[],"Heng Wang":[],"Weidong Cai":[]},affiliations:[],pdf:"/papers/2022/9.pdf"}),(0,f.Y)(T,{title:"Benchmarking Augmentation Methods for Learning Robust Navigation Agents: The Winning Entry of the 2021 iGibson Challenge",abstract:"While impressive progress has been made for teaching embodied agents to navigate static environments using vision, much less progress has been made on more dynamic environments that may include moving pedestrians or movable obstacles. In this study, we aim to benchmark different augmentation techniques for improving the agent's performance in these challenging environments. We show that adding several dynamic obstacles into the scene during training confers significant improvements in test-time generalization, achieving much higher success rates than baseline agents. We find that this approach can also be combined with image augmentation methods to achieve even higher success rates. Additionally, we show that this approach is also more robust to sim-to-sim transfer than image augmentation methods. Finally, we demonstrate the effectiveness of this dynamic obstacle augmentation approach by using it to train an agent for the 2021 iGibson Challenge at CVPR, achieving 1st place for Interactive Navigation.",authors:{"Naoki Yokoyama":[],"Qian Luo":[],"Dhruv Batra":[],"Sehoon Ha":[]},affiliations:[],pdf:"/papers/2022/10.pdf"}),(0,f.Y)(T,{title:"Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale",abstract:"We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments – (1) ObjectGoal Navigation (e.g. ‘find & go to a chair’) and (2) Pick&Place (e.g. ‘find mug, pick mug, find counter, place mug on counter’). First, we develop a virtual teleoperation data-collection infrastructure – connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for ObjectNav and 12k demonstrations for Pick&Place, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots. Second, we use this data to answer the question – how does large-scale imitation learning (IL) (which has not been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. This effectively establishes an ‘exchange rate’ – a single human demonstration appears to be worth ∼4 agent-gathered ones. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On Pick&Place, the comparison is starker – IL agents achieve ∼18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning. Project page: https://ram81.github.io/projects/habitat-web.",authors:{"Ram Ramrakhya":[],"Eric Undersander":[],"Dhruv Batra":[],"Abhishek Das":[]},affiliations:[],pdf:"/papers/2022/11.pdf"}),(0,f.Y)(T,{title:"IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents",abstract:"We present the IGLU Gridworld: a reinforcement learning environment for building and evaluating language conditioned embodied agents in a scalable way. The environment features visual agent embodiment, interactive learning through collaboration, language conditioned RL, and combinatorically hard task (3d blocks building) space.",authors:{"Artem Zholus":[],"Alexey Skrynnik":[],"Shrestha Mohanty":[],"Zoya Volovikova":[],"Julia Kiseleva":[],"Arthur Szlam":[],"Marc-Alexandre Côté":[],"Aleksandr Panov":[]},affiliations:[],pdf:"/papers/2022/12.pdf"}),(0,f.Y)(T,{title:"SAMPLE-HD: Simultaneous Action and Motion Planning Learning Environment",abstract:"Humans exhibit incredibly high levels of multi-modal understanding - combining visual cues with read, or heard knowledge comes easy to us and allows for very accurate interaction with the surrounding environment. Various simulation environments focus on providing data for tasks related to scene understanding, question answering, space exploration, visual navigation. In this work, we are providing a solution to encompass both, visual and behavioural aspects of simulation in a new environment for learning interactive reasoning in manipulation setup. SAMPLE-HD environment allows to generate various scenes composed of small household objects, to procedurally generate language instructions for manipulation, and to generate ground truth paths serving as training data.",authors:{"Michal Nazarczuk":[],"Tony Ng":[],"Krystian Mikolajczyk":[]},affiliations:[],pdf:"/papers/2022/13.pdf"}),(0,f.Y)(T,{title:"Human Instruction Following: Graph Neural Network Guided Object Navigation",abstract:"Home-assistant robots (e.g., mobile manipulator) following human instruction is a long-standing topic of research whose main challenge comes from the interpretation of diverse instructions and dynamically-changing environments. This paper proposes a hybrid planner for parsing human instruction and task planning, and a graph-based object navigation to search unknown objects by exploiting a partially known semantic map. We present preliminary evaluations of human instruction parsing and object-to-object link prediction based on graph neural network prediction, and demonstrate their effectiveness in human instruction following tasks.",authors:{"Hongyi Chen":[],"Letian Wang":[],"Yuhang Yao":[],"Ye Zhao":[],"Patricio Vela":[]},affiliations:[],pdf:"/papers/2022/14.pdf"}),(0,f.Y)(T,{title:"A Planning based Neural-Symbolic Approach for Embodied Instruction Following",abstract:"The ALFRED environment features an embodied agent following instructions and accomplishing tasks in simulated home environments. However, end-to-end deep learning methods struggle at these tasks due to long-horizon and sparse rewards. In this work, we propose a principled neural-symbolic approach combining symbolic planning and deep-learning methods for visual perception and NL processing. The symbolic model is enriched as exploration progress until a full plan can be obtained. New perceptions are added to a discrete graph representation that is used for producing new planning problems. Empirical results demonstrate that our approach can achieve high scalability with SOTA performance of 36.04% unseen success rate in the ALFRED benchmark. Our work builds a foundation for a neural-symbolic approach that can act in unstructured environments when the set of skills and possible relationships is known.",authors:{"Xiaotian Liu":[],"Hector Palacios":[],"Christian Muise":[]},affiliations:[],pdf:"/papers/2022/15.pdf"}),(0,f.Y)(T,{title:"Modality-invariant Visual Odometry for Indoor Navigation",abstract:'Successful indoor navigation is a crucial skill for many robots. This fundamental ability has been extensively studied through the task of PointGoal navigation in simulated environments. With noisy observations and actuation, the setting becomes more realistic and previously successful agents fail dramatically. Visual Odometry has shown to be a practical substitute for GPS+compass and can effectively localize the agent from visual observations. With the availability of multiple sensors and estimators, the question naturally arises of how to make the most use of multiple input modalities. When having access to multiple modalities, the predictions of naive multi-modal approaches can be dominated by a single one, impeding overall robustness. Recent methods are modality-specific and can not deal with "privileged" modalities, e.g., irregular or no access to depth during test time. We propose the Visual Odometry Transformer, a novel approach to multi-modal Visual Odometry based on Vision Transformers that successfully replaces GPS+compass. Our experiments show that the model can deal with limited availability of modalities during test time by implicitly learning a representation invariant to the availability of input modalities.',authors:{"Marius Memmel":[],"Amir Zamir":[]},affiliations:[],pdf:"/papers/2022/17.pdf"}),(0,f.Y)(T,{title:"Role of reward shaping in object-goal navigation",abstract:"Deep reinforcement learning approaches have been a popular method for visual navigation tasks in the computer vision and robotics community of late. In most cases, the reward function has a binary structure, i.e., a large positive reward is provided when the agent reaches goal state, and a negative step penalty is assigned for every other state in the environment. A sparse signal like this makes the learning process challenging, specially in big environments, where a large number of sequential actions need to be taken to reach the target. We introduce a reward shaping mechanism which gradually adjusts the reward signal based on distance to the goal. Detailed experiments conducted using the AI2-THOR simulation environment demonstrate the efficacy of the proposed approach for object-goal navigation tasks.",authors:{"Srirangan Madhavan":[],"Anwesan Pal":[],"Henrik Christensen":[]},affiliations:[],pdf:"/papers/2022/18.pdf"}),(0,f.Y)(T,{title:"Bridging the Gap between Events and Frames through Unsupervised Domain Adaptation",abstract:"Event cameras are novel sensors with outstanding properties such as high temporal resolution and high dynamic range. However, event-based vision has been held back by the shortage of labeled datasets due to the novelty of event cameras. To overcome this drawback, we propose a task transfer method to train models directly with labeled images and unlabeled event data. We leverage the generative event model to split event features into content and motion features. Thus, our approach unlocks the vast amount of existing image datasets for the training of event-based neural networks. Our task transfer method outperforms methods targeting Unsupervised Domain Adaptation for object detection by 0.26 mAP and classification by 2.7% accuracy.",authors:{"Nico Messikommer":[],"Daniel Gehrig":[],"Mathias Gehrig":[],"Davide Scaramuzza":[]},affiliations:[],pdf:"/papers/2022/19.pdf"}),(0,f.Y)(T,{title:"BEHAVIOR in Habitat 2.0: Simulator-Independent Logical Task Description for Benchmarking Embodied AI Agents",abstract:"Robots excel in performing repetitive and precision-sensitive tasks in controlled environments such as warehouses and factories, but have not been yet extended to embodied AI agents providing assistance in household tasks. Inspired by the catalyzing effect that benchmarks have played in the AI fields such as computer vision and natural language processing, the community is looking for new benchmarks for embodied AI. Prior work in embodied AI benchmark defines tasks using a different formalism, often specific to one environment, simulator or domain, making it hard to develop general and comparable solutions. In this work, we bring a subset of BEHAVIOR activities into Habitat 2.0 to benefit from its fast simulation speed, as a first step towards demonstrating the ease of adapting activities defined in the logic space into different simulators.",authors:{"Ziang Liu":[],"Roberto Martin-Martin":[],"Fei Xia":[],"Jiajun Wu":[],"Li Fei-Fei":[]},affiliations:[],pdf:"/papers/2022/22.pdf"}),(0,f.Y)(T,{title:"Language Guided Meta-Control for Embodied Instruction Following",abstract:"Embodied Instruction Following (EIF) is a challenging problem requiring an agent to infer a sequence of actions to achieve a goal environment state from complex language and visual inputs. We propose a generalised Language Guided Meta-Controller (LMC) for better language grounding in the large action space of the embodied agent. We additionally propose an auxiliary reasoning loss to improve `conceptual grounding' of the agent. Our empirical validation shows that our approach outperforms strong baselines on the Execution from Dialogue History (EDH) benchmark from the TEACh benchmark.",authors:{"Divyam Goel":[],"Kunal Pratap Singh":[],"Jonghyun Choi":[]},affiliations:[],pdf:"/papers/2022/23.pdf"}),(0,f.Y)(T,{title:"Learning to navigate in interactive Environments with the transformer-based memory",abstract:"Substantial progress has been achieved in embodied visual navigation based on reinforcement learning (RL). These studies presume that the environment is stationary where all the obstacles are static. However, in real cluttered scenes, interactable objects (e.g. shoes and boxes) blocking the way of robots makes the environment non-stationary. We formulate this interactive visual navigation as a Partial Observed Markov Decision Problem. To handle it, we propose a transformer encoder to learn a belief state which captures the long spatial-temporal dependencies of the aggregated observations in the memory. However, leveraging the transformer architecture in the RL settings is highly unstable. We propose a surrogate objective to predict the next waypoint, which facilitates the representation learning and bootstrap the RL. We demonstrate our method in the iGibson environment and experimental results show a significant improvement over the interactive Gibson benchmark and the related recurrent RL policy both in the validation seen scenes and the test unseen scenes.",authors:{"Weiyuan Li":[],"Ruoxin Hong":[],"Jiwei Shen":[],"Yue Lu":[]},affiliations:[],pdf:"/papers/2022/24.pdf"}),(0,f.Y)(T,{title:"Learning to navigate in interactive Environments with the transformer-based memory",abstract:"Substantial progress has been achieved in embodied visual navigation based on reinforcement learning (RL). These studies presume that the environment is stationary where all the obstacles are static. However, in real cluttered scenes, interactable objects (e.g. shoes and boxes) blocking the way of robots makes the environment non-stationary. We formulate this interactive visual navigation as a Partial Observed Markov Decision Problem. To handle it, we propose a transformer encoder to learn a belief state which captures the long spatial-temporal dependencies of the aggregated observations in the memory. However, leveraging the transformer architecture in the RL settings is highly unstable. We propose a surrogate objective to predict the next waypoint, which facilitates the representation learning and bootstrap the RL. We demonstrate our method in the iGibson environment and experimental results show a significant improvement over the interactive Gibson benchmark and the related recurrent RL policy both in the validation seen scenes and the test unseen scenes.",authors:{"Weiyuan Li":[],"Ruoxin Hong":[],"Jiwei Shen":[],"Yue Lu":[]},affiliations:[],pdf:"/papers/2022/24.pdf"}),(0,f.Y)(T,{title:"ET tu, CLIP? Addressing Common Object Errors for Unseen Environments",abstract:"We introduce a simple method that employs pre-trained CLIP encoders to enhance model generalization in the ALFRED task. In contrast to previous literature where CLIP replaces the visual encoder, we suggest using CLIP as an additional module through an auxiliary object detection objective. We validate our method on the recently proposed Episodic Transformer architecture and demonstrate that incorporating CLIP improves task performance on the unseen validation set. Additionally, our analysis results support that CLIP especially helps with leveraging object descriptions, detecting small objects, and interpreting rare words.",authors:{"Jimin Sun":[],"Ye Won Byun":[],"Shahriar Noroozizadeh":[],"Rosanna M Vitiello":[],"Cathy L Jiao":[]},affiliations:[],pdf:"/papers/2022/20.pdf"}),(0,f.Y)(T,{title:"Housekeep: Tidying Virtual Households using Commonsense Reasoning",abstract:"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We show that our baseline agent generalizes to rearranging unseen objects in unknown environments.",authors:{"Yash Mukund":[],"Arun Ramachandran":[],"Sriram Yenamandra":[],"Igor Gilitschenski":[],"Dhruv Batra":[],"Andrew Szot":[],"Harsh Agrawal":[]},affiliations:[],pdf:"/papers/2022/21.pdf"}),(0,f.Y)(T,{title:"Learning Value Functions from Undirected State-only Experience",abstract:"This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods.",authors:{"Matthew Chang":[],"Arjun Gupta":[],"Saurabh Gupta":[]},affiliations:[],pdf:"/papers/2022/16.pdf"})];const P=e=>(0,f.Y)("span",{css:(0,f.AH)("color:",d.A.gray7,";","","","")},e.time),j=function(e){var a,t=e.length;for(;0!==t;)a=Math.floor(Math.random()*t),t--,[e[t],e[a]]=[e[a],e[t]];return e}((0,o.A)(Array(M.length).keys()));var W={name:"5ou1pc",styles:"width:15px;margin-right:5px"},H={name:"1guecoj",styles:"display:inline-block;border-radius:0px 10px 0px 10px;padding-left:10px;padding-right:10px;margin-top:3px;padding-top:3px;padding-bottom:4px;background-color:#4a154b;transition-duration:0.15s;color:white;&:hover{cursor:pointer;filter:contrast(1.25);}>span,>img{vertical-align:middle;}"};function E(){return(0,f.Y)("div",null,(0,f.Y)("a",{href:"//join.slack.com/t/embodied-aiworkshop/shared_invite/zt-s6amdv5c-gBZQZ7YSktrD_tMhQDjDfg",target:"_blank"},(0,f.Y)("div",{css:H},(0,f.Y)("img",{src:m.A,css:W})," ",(0,f.Y)("span",null,"Ask questions on ",(0,f.Y)("b",null,"Slack")))))}var z={name:"rzxmlp",styles:"display:grid;grid-gap:2%;grid-row-gap:20px;grid-template-columns:49% 49%;@media (max-width: 600px){grid-template-columns:100%;}"},O={name:"1azakc",styles:"text-align:center"},G={name:"jdiuzp",styles:"margin-top:25px;margin-bottom:50px"},F={name:"1qs5g6e",styles:"margin-left:0px;margin-top:20px"},N={name:"8lu43",styles:'width:130%;background-repeat:no-repeat;padding-top:70.25%;margin-top:-25px;margin-left:-15%;margin-bottom:-15px;background-image:url("/images/cvpr2022/cover.jpg");background-size:cover;background-position:center'};function V(e){let{data:a}=e;const{0:t,1:o}=(0,s.useState)(R());(0,s.useEffect)((()=>{const e=()=>o(R());return window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)}));const u=[{challenge:v["AI2-THOR Rearrangement"],key:"ai2thor-rearrangement",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:v.ALFRED,key:"alfred",task:"Vision-and-Language Interaction",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB",stochasticAcuation:""},{challenge:v.Habitat,key:"habitat-objectNav",task:"ObjectNav",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:v.iGibson,key:"igibson-in",task:"Interactive Navigation",interactiveActions:"✓",simulationPlatform:"iGibson",sceneDataset:"iGibson",actionSpace:"Continuous",observations:"RGB-D",stochasticAcuation:"✓"},{challenge:v.iGibson,key:"igibson-social-navigation",task:"Social Navigation",interactiveActions:"✓",simulationPlatform:"iGibson",sceneDataset:"iGibson",actionSpace:"Continuous",observations:"RGB-D",stochasticAcuation:"✓"},{challenge:v.MultiOn,key:"multion",task:"Multi-Object Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:""},{challenge:v["Robotic Vision Scene Understanding"],key:"rvsu",task:"Rearrangement (SCD)",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"✓"},{challenge:v["Robotic Vision Scene Understanding"],key:"rvsu-2",task:"Semantic SLAM",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"Partially"},{challenge:v["RxR-Habitat"],key:"rxr",task:"Vision-and-Language Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D",actionSpace:"Discrete",stochasticAcuation:""},{challenge:v.SoundSpaces,key:"soundspaces",task:"Audio Visual Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D, Audio Waveform",actionSpace:"Discrete",stochasticAcuation:""},{challenge:v["TDW-Transport"],key:"tdw",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"TDW",sceneDataset:"TDW",observations:"RGB-D, Metadata",actionSpace:"Discrete",stochasticAcuation:"✓"},{challenge:v.TEACh,key:"teach",task:"Vision-and-Dialogue Interaction",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",observations:"RGB",actionSpace:"Discrete, Text Generation",stochasticAcuation:""}],m=g().tz("2022-05-17 04:59","America/Los_Angeles"),y=g()(),w=g().duration(m.diff(y));Math.ceil(w.asHours()%24),Math.floor(w.asDays());return(0,f.Y)(c.A,{headerGradient:"radial-gradient(#090617, #090617)",headerStyle:(0,f.AH)("color:",d.A.dark.gold10,"!important;button{&:hover{color:",d.A.dark.gold9,"!important;}}","","",""),imageContent:{css:N},conference:"CVPR 2022",rightSide:(0,f.Y)(h.NT,{conference:"CVPR 2022",challengeData:Object.values(v)})},(0,f.Y)(l.wn,{title:"Overview"},(0,f.Y)("p",null,"Within the last decade, advances in deep learning, coupled with the creation of large, freely available datasets (e.g., ImageNet), have resulted in remarkable progress in the computer vision, NLP, and broader AI communities. This progress has enabled models to begin to obtain superhuman performance on a wide variety of passive tasks. However, this progress has also enabled a paradigm shift that a growing collection of researchers take aim at: the creation of an embodied agent (e.g., a robot) which learns, through interaction and exploration, to creatively solve challenging tasks within its environment."),(0,f.Y)("p",null,"The goal of this workshop is to bring together researchers from the fields of computer vision, language, graphics, and robotics to share and discuss the current state of intelligent agents that can:"),(0,f.Y)("ul",null,(0,f.Y)("li",null,(0,f.Y)("b",null,"See"),": perceive their environment through vision or other senses."),(0,f.Y)("li",null,(0,f.Y)("b",null,"Talk"),": hold a natural language dialog grounded in their environment."),(0,f.Y)("li",null,(0,f.Y)("b",null,"Listen"),": understand and react to audio input anywhere in a scene."),(0,f.Y)("li",null,(0,f.Y)("b",null,"Act"),": navigate and interact with their environment to accomplish goals."),(0,f.Y)("li",null,(0,f.Y)("b",null,"Reason"),": consider and plan for the long-term consequences of their actions.")),(0,f.Y)("p",null,"The Embodied AI 2022 workshop will be held in conjunction with CVPR 2022. It will feature a host of invited talks covering a variety of topics in Embodied AI, many exciting challenges, a poster session, and panel discussions."),(0,f.Y)(I,{actionIdentifier:"1FAIpQLSeIZrn-tk7Oain2R8gc_Q0HzLMLQ9XXwqu3KecK_E5kALpiug",entryNumber:1834823104})),(0,f.Y)(l.wn,{title:"Timeline"},(0,f.Y)(r.A,{progressDot:!0,current:0,direction:"vertical"},(0,f.Y)(b,{title:"CVPR Workshop",description:(0,f.Y)(s.Fragment,null,(0,f.Y)("a",{href:"/images/cvpr2022/map-location.jpg",target:"_blank"},"Room 224, New Orleans Ernest M. Morial Conventinon Center")," ",(0,f.Y)("br",null),"June 19, 2022",(0,f.Y)("br",null),"9:00 AM - 5:30 PM CT ",(0,f.Y)("br",null),(0,f.Y)("span",{css:(0,f.AH)("color:",d.A.gray7,";","","","")},"Tentative Schedule:"),(0,f.Y)("div",{css:F},(0,f.Y)(i.A,null,(0,f.Y)(i.A.Item,null,"Workshop Introduction",(0,f.Y)("br",null),(0,f.Y)(P,{time:"9:00 AM CT"})),(0,f.Y)(i.A.Item,null,"Navigation & Understanding Challenge Presentations",(0,f.Y)("br",null),"(MultiON, SoundSpaces, RxR-Habitat, RVSU)",(0,f.Y)("br",null),(0,f.Y)(P,{time:"9:10 AM CT"})),(0,f.Y)(i.A.Item,null,"Navigation & Understanding Challenge Q&A Panel",(0,f.Y)("br",null),"(MultiON, SoundSpaces, RxR-Habitat, RVSU)",(0,f.Y)("br",null),(0,f.Y)(P,{time:"10:00 AM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["Google AI"],name:"Carolina Parada",fixedImg:a.carolina.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"10:30 AM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["Allen Institute for AI"],name:"Roozbeh Mottaghi",fixedImg:a.roozbeh.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"11:00 AM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["GaTech","FAIR"],name:"Dhruv Batra",fixedImg:a.dhruv.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"11:30 AM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Accepted Papers Poster Session",(0,f.Y)("br",null),(0,f.Y)(P,{time:"12:00 PM CT"})),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["Carnegie Mellon"],name:"Katerina Fragkiadaki",fixedImg:a.katerina.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"1:30 PM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["Stanford"],name:"Fei-Fei Li",fixedImg:a.feifei.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"2:00 PM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Talk",(0,f.Y)(p.Speaker,{organizations:["Berkeley"],name:"Jitendra Malik",fixedImg:a.jitendra.childImageSharp.fixed,noMargin:!0}),(0,f.Y)(P,{time:"2:30 PM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Interaction Challenge Presentations",(0,f.Y)("br",null),"AI2-Rearrangement, ALFRED, TEACh",(0,f.Y)("br",null),(0,f.Y)(P,{time:"3:00 PM CT"})),(0,f.Y)(i.A.Item,null,"Interaction Challenge Q&A Panel",(0,f.Y)("br",null),(0,f.Y)(P,{time:"4:00 PM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Invited Speaker Panel",(0,f.Y)("br",null),(0,f.Y)(P,{time:"4:30 PM CT"}),(0,f.Y)(E,null)),(0,f.Y)(i.A.Item,null,"Workshop Concludes",(0,f.Y)("br",null),(0,f.Y)(P,{time:"5:30 PM CT"})))))}),(0,f.Y)(b,{title:"Challenge Submission Deadlines",description:"May 2022. Check each challenge for the specific date."}),(0,f.Y)(b,{title:"Paper Submission Deadline",description:"May 16, 2022 (Anywhere on Earth)"}),(0,f.Y)(b,{title:"Workshop Announced",description:"Feb 14, 2022"}))),(0,f.Y)(l.wn,{title:"Challenges"},(0,f.Y)("p",null,"The Embodied AI 2022 workshop is hosting many exciting challenges covering a wide range of topics such as rearrangement, visual navigation, vision-and-language, and audio-visual navigation. More details regarding data, submission instructions, and timelines can be found on the individual challenge websites."),(0,f.Y)("p",null,"Challenge winners will be given the opportunity to present a talk at the workshop. Since many challenges can be grouped into similar tasks, we encourage participants to submit models to more than 1 challenge. The table below describes, compares, and links each challenge."),(0,f.Y)(n.A,{scroll:{x:"1500px"},css:G,sticky:!0,columns:[{title:(0,f.Y)(s.Fragment,null,"Challenge"),dataIndex:"challenge",key:"challenge",fixed:t>650?"left":""},{title:(0,f.Y)(s.Fragment,null,"Task"),dataIndex:"task",key:"task",sorter:(e,a)=>e.task.localeCompare(a.task),sortDirections:["ascend","descend"]},{title:(0,f.Y)(s.Fragment,null,"Interactive Actions?"),dataIndex:"interactiveActions",key:"interactiveActions",sorter:(e,a)=>e.interactiveActions.localeCompare(a.interactiveActions),sortDirections:["descend","ascend"],width:200},{title:(0,f.Y)(s.Fragment,null,"Simulation Platform"),dataIndex:"simulationPlatform",key:"simulationPlatform",sorter:(e,a)=>e.simulationPlatform.localeCompare(a.simulationPlatform),sortDirections:["ascend","descend"],width:200},{title:(0,f.Y)(s.Fragment,null,"Scene Dataset"),dataIndex:"sceneDataset",key:"sceneDataset",sorter:(e,a)=>e.sceneDataset.localeCompare(a.sceneDataset),sortDirections:["ascend","descend"],width:180},{title:(0,f.Y)(s.Fragment,null,"Observations"),key:"observations",dataIndex:"observations",sorter:(e,a)=>e.observations.localeCompare(a.observations),sortDirections:["ascend","descend"],width:170},{title:(0,f.Y)(s.Fragment,null,"Stochastic Acuation?"),key:"stochasticAcuation",dataIndex:"stochasticAcuation",sorter:function(e,a){let t="✓"===e.stochasticAcuation?"Z":e.stochasticAcuation,n="✓"===a.stochasticAcuation?"Z":a.stochasticAcuation;return t.localeCompare(n)},sortDirections:["descend","ascend"]},{title:(0,f.Y)("div",{css:O},"Action Space"),key:"actionSpace",dataIndex:"actionSpace",sorter:(e,a)=>e.actionSpace.localeCompare(a.actionSpace),sortDirections:["ascend","descend"],width:165}],dataSource:u,pagination:!1})),(0,f.Y)(l.wn,{title:"Call for Papers"},(0,f.Y)("p",null,"We invite high-quality 2-page extended abstracts in relevant areas, such as:",(0,f.Y)("ul",null,(0,f.Y)("li",null,"Simulation Environments"),(0,f.Y)("li",null,"Visual Navigation"),(0,f.Y)("li",null,"Rearrangement"),(0,f.Y)("li",null,"Embodied Question Answering"),(0,f.Y)("li",null,"Simulation-to-Real Transfer"),(0,f.Y)("li",null,"Embodied Vision & Language")),"Accepted papers will be presented as posters or spotlight talks at the workshop. These papers will be made publicly available in a non-archival format, allowing future submission to archival journals or conferences. Paper submissions do not have to be anononymized. Per"," ",(0,f.Y)("a",{href:"https://docs.google.com/document/d/1JWVoTitdS5UhktYNI2KyRP8JdDplawr_Zwm6R0ymQwI",target:"_blank"},"CVPR rules")," ","regarding workshop papers, at least one author must register for CVPR using an in-person registration."),(0,f.Y)(l.Wo,{title:"Submission"},(0,f.Y)("p",null,"The submission deadline is May 16th (",(0,f.Y)("a",{href:"//time.is/Anywhere_on_Earth"},"Anywhere on Earth"),"). Papers should be no longer than 2 pages (excluding references) and styled in the"," ",(0,f.Y)("a",{href:"//cvpr2022.thecvf.com/author-guidelines",target:"_blank"},"CVPR format"),". Paper submissions are now closed.")),(0,f.Y)(l.Wo,{title:"Accepted Papers"},(0,f.Y)("p",null,(0,f.Y)("b",null,"Note.")," The order of the papers is randomized each time the page is refreshed."),(0,f.Y)("div",{css:z},j.map((e=>M[e]))))),(0,f.Y)(l.wn,{title:"Organizers"},"The Embodied AI 2022 workshop is a joint effort by a large set of researchers from a variety of organizations. They are listed below in alphabetical order.",(0,f.Y)(l.Wo,{title:"Organizing Committee"},(0,f.Y)(p.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2022.organizers.filter((e=>!0===e.oc)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a})),(0,f.Y)(l.Wo,{title:"Challenge Organizers"},(0,f.Y)(p.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2022.organizers.filter((e=>!0===e.challenge)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a})),(0,f.Y)(l.Wo,{title:"Scientific Advisory Board"},(0,f.Y)(p.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2022.organizers.filter((e=>!0===e.sab)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a}))))}}}]);
//# sourceMappingURL=0cc94637-100e574b1873c0a67f67.js.map