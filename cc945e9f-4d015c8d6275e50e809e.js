"use strict";(self.webpackChunkembodied_ai_workshop=self.webpackChunkembodied_ai_workshop||[]).push([[681],{3849:function(e,a,t){t.r(a),t.d(a,{default:function(){return q}});t(3960);var n=t(1381),i=(t(1651),t(9304)),o=t(436),r=(t(9412),t(7262)),s=t(6540),l=t(6568),c=t(3020),d=t(2316),h=t(2158),u=t(4743),g=t.n(u),m=t(4333),p=t(853),f=t(7850),b=t(1013);const{Step:v}=r.A;const w={"AI2-THOR Rearrangement":(0,b.Y)("a",{href:"https://github.com/allenai/ai2thor-rearrangement",target:"_blank"},"AI2-THOR Rearrangement"),DialFRED:(0,b.Y)("a",{href:"https://eval.ai/web/challenges/challenge-page/1859/overview",target:"_blank"},"DialFRED"),Habitat:(0,b.Y)("a",{href:"//aihabitat.org/challenge/2023/",target:"_blank"},"Habitat"),"Language Interaction":(0,b.Y)("a",{href:"//askforalfred.com/EAI23",target:"_blank"},"Language Interaction"),ManiSkill:(0,b.Y)("a",{href:"https://sapien.ucsd.edu/challenges/maniskill",target:"_blank"},"ManiSkill"),MultiOn:(0,b.Y)("a",{href:"http://multion-challenge.cs.sfu.ca",target:"_blank"},"MultiON"),"Robotic Vision Scene Understanding":(0,b.Y)("a",{href:"https://nikosuenderhauf.github.io/roboticvisionchallenges/cvpr2023",target:"_blank"},"Robotic Vision Scene Understanding"),"RxR-Habitat":(0,b.Y)("a",{href:"//ai.google.com/research/rxr/habitat",target:"_blank"},"RxR-Habitat"),SoundSpaces:(0,b.Y)("a",{href:"//soundspaces.org/challenge",target:"_blank"},"SoundSpaces"),"TDW-Transport":(0,b.Y)("a",{href:"http://tdw-transport.csail.mit.edu",target:"_blank"},"TDW-Transport")};var y={name:"14y6t8x",styles:"display:none!important"},k={name:"99y604",styles:"margin-top:5px;color:#8c8c8c"},Y={name:"rowx5j",styles:"border-radius:5px;box-shadow:0px 0px 2px 0px #2b4acb;display:inline-block;margin:auto;*{padding-top:3px;padding-bottom:5px;}"},A={name:"nl3i1y",styles:"vertical-align:middle;display:inline-block;margin-top:6px;margin-left:5px"},x={name:"13sa3rf",styles:'font-weight:bold;font-size:25px;color:"#2b4acb";vertical-align:middle;display:inline-block'},I={name:"pr10xp",styles:"margin-bottom:10px"},S={name:"uewfz3",styles:"text-align:center;margin-top:60px;margin-bottom:60px"};function R(e){const[a,t]=s.useState(!1),[n,i]=s.useState(!1),[o,r]=s.useState(""),l=/^(([^<>()[\]\\.,;:\s@"]+(\.[^<>()[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/.test(String(o).toLowerCase());return(0,b.Y)("div",{css:S},(0,b.Y)("form",{encType:"text/plain",action:l?"https://docs.google.com/forms/d/e/"+e.actionIdentifier+"/formResponse?usp=pp_url&entry."+e.entryNumber+"="+o:"",target:"hidden_iframe"+e.actionIdentifier,onSubmit:()=>!!l&&t(!0),method:"post"},(0,b.Y)("div",{css:I},(0,b.Y)("div",{css:x},"Sign Up for Updates"),(0,b.Y)("div",{css:A})),a?(0,b.Y)("div",null,"Thanks for signing up!"):(0,b.Y)(s.Fragment,null,(0,b.Y)("div",{css:Y},(0,b.Y)("input",{type:"email",autoComplete:"off",placeholder:"email",name:"entry."+e.entryNumber,id:"entry."+e.entryNumber,onFocus:()=>i(!0),onBlur:()=>i(!1),onChange:e=>r(e.target.value),value:o,css:(0,b.AH)("background-color:transparent;transition-duration:0.3s;box-shadow:0px 0px 1px 2px ",n||l||""==o?"transparent":"#ff7875",";border:none;width:350px;@media (max-width: 500px){width:55vw;}border-radius:5px;padding-left:8px;","","","")}),(0,b.Y)("input",{type:l?"submit":"button",value:"Sign Up",onClick:()=>!!l,css:(0,b.AH)("background-color:transparent;border:none;font-weight:600;transition-duration:0.3s;color:",l?"#2b4acb":"#2b4acb88",";padding-top:3px;padding-right:12px;padding-left:10px;&:hover{cursor:",l?"pointer":"default",";}","","","")})),(0,b.Y)("div",{css:k},"You can unsubscribe at any time."))),(0,b.Y)("iframe",{name:"hidden_iframe"+e.actionIdentifier,id:"hidden_iframe"+e.actionIdentifier,css:y}))}function M(){if("undefined"==typeof window)return 800;const{innerWidth:e}=window;return e}var D={name:"nkt64x",styles:"margin-right:10px"};function P(e){return(0,b.Y)("a",{href:e.url,target:"_blank",css:D},(0,b.Y)("div",{css:(0,b.AH)("display:inline-block;border:1px solid ",d.A.gray5,";background-color:",d.A.gray2,";padding-left:7px;padding-right:7px;border-radius:5px;transition-duration:0.15s;>span{vertical-align:middle;}&:hover{background-color:",d.A.gray4,";border:1px solid ",d.A.gray6,";}","","","")},(0,b.Y)("span",{css:(0,b.AH)("margin-left:5px;color:",d.A.gray10,";","","","")},e.text)))}function T(e){const[a,t]=s.useState(!1);let n;return n=-1===e.text.indexOf(" ",250)?(0,b.Y)("div",null,e.text):(0,b.Y)("div",null,a?e.text+" ":e.text.indexOf(". ")+2>250?e.text.slice(0,e.text.indexOf(". ")+2):e.text.slice(0,250)+"... ",(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>t((e=>!e))},"[",a?"Collapse":"Expand","]")),(0,b.Y)("div",{css:(0,b.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:25px;}","","","")},n)}var C={name:"4ozr74",styles:"position:absolute;bottom:10px;width:calc(100% - 40px);padding-top:5px"};function E(e){const[a,t]=s.useState(!1);let n;return n=-1===e.abstract.indexOf(" ",250)?(0,b.Y)("div",null,e.abstract):(0,b.Y)("div",null,a?e.abstract+" ":e.abstract.slice(0,e.abstract.indexOf(". ")+2),(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>t((e=>!e))},"[",a?"Collapse":"Expand","]")),(0,b.Y)("div",{css:(0,b.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:250px;}","","","")},(0,b.Y)("a",{href:e.pdf,target:"_blank"},(0,b.Y)("div",{css:(0,b.AH)("font-weight:600;line-height:20px;color:",d.A.light.blue7,";font-size:15px;transition-duration:0.15s;&:hover{color:",d.A.light.blue6,";}","","","")},e.title)),(0,b.Y)("div",{css:(0,b.AH)("margin-bottom:8px;color:",d.A.gray8,";line-height:20px;font-size:13px;","","","")},Object.keys(e.authors).map(((a,t)=>(0,b.Y)(s.Fragment,null,(0,b.Y)("span",null,a),(0,b.Y)("sup",null),t!==Object.keys(e.authors).length-1?", ":"")))),n,(0,b.Y)("div",{css:C},(0,b.Y)(P,{text:"PDF",url:e.pdf}),e.poster?(0,b.Y)(P,{text:"Poster",url:e.poster}):(0,b.Y)(s.Fragment,null)))}let L=[(0,b.Y)(E,{title:"Boosting Outdoor Vision-and-Language Navigation with On-the-route Objects",abstract:"Outdoor Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate using real-world urban environment data and natural language instructions. Current outdoor VLN models tend to overlook crucial navigation roles, such as objects that serve as landmarks for accurate turn and stop locations. This occurs because they primarily focus on panoramas and instructions, while disregarding objects that provide essential information for accurate decisions, such as identifying correct turn and stop locations, which humans naturally use as landmarks in unfamiliar places. In this paper, we propose the Object-Attention VLN (OAVLN) model, inspired by human navigation, which focuses on relevant on-the-route objects. Our model outperforms previous methods across all evaluation metrics on two benchmark datasets, Touchdown and map2seq.",authors:{"Yanjun Sun":[],"Yue Qiu":[],"Yoshimitsu Aoki":[],"Hirokatsu Kataoka":[]},affiliations:[],pdf:"/papers/2023/1.pdf"}),(0,b.Y)(E,{title:"Generalizing Skill Embeddings Across Body Shapes for Physically Simulated Characters",abstract:"Recent progress in physics-based character animation has enabled learning diverse skills from large motion capture datasets. However, most often only a single character shape is considered. On the other hand, work on controlling various body shapes with one policy is limited to few motions. In this paper, we first evaluate the generalization capabilities of latent skill embeddings on physicsbased character control for varying body shapes. We then propose two strategies to learn a single policy that can generalize across different body shapes. In our experiments, we show that these simple but effective strategies significantly improve the performance over state-of-the-art, without having to retrain the skill embeddings from scratch.",authors:{"Sammy Christen":[],"Nina Schmid":[],"Otmar Hilliges":[]},affiliations:[],pdf:"/papers/2023/2.pdf"}),(0,b.Y)(E,{title:"Question Generation to Disambiguate Referring Expressions in 3D Environment",abstract:"Our paper presents a novel task and method for question generation, aimed to disambiguate referring expressions within 3D indoor environments (3D-REQ). Referring to objects using natural language is a fundamental aspect of human communication, and an essential capability for robots in various applications such as room organization. However, human instructions can sometimes be ambiguous, which poses challenges to existing research on visual grounding in 3D environments that assumes referring expressions can uniquely identify objects. To address this issue, we introduce a method inspired by human communication, where ambiguities are resolved by asking questions. Our approach predicts the positions of candidate objects that satisfy given referring expressions in a 3D environment and generates appropriate questions to narrow down the target objects. To facilitate this, we have constructed a new dataset (3D-REQ), containing input referring expressions with ambiguities and point clouds and output bounding boxes of candidate objects and questions to eliminate ambiguities. To our knowledge, 3D-REQ is the first effort to tackle the challenge of ambiguous referring expressions in 3D object grounding.",authors:{"Fumiya Matsuzawa":[],"Ryo Nakamura":[],"Kodai Nakashima":[],"Yue Qiu":[],"Hirokatsu Kataoka":[],"Yutaka Satoh":[]},affiliations:[],pdf:"/papers/2023/3.pdf"}),(0,b.Y)(E,{title:"Audio Visual Language Maps for Robot Navigation",abstract:"While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. We propose AVLMaps, a 3D spatial map representation that stores cross-modal information from audio, visual, and language cues. AVLMaps fuse features from pre-trained multimodal foundation models into a centralized voxel grid. This enables robots to index goals in the map based on multimodal queries, such as textual descriptions, images, or audio snippets of landmarks. AVLMaps allow for zero-shot multimodal goal navigation and perform better than alternatives in ambiguous scenarios. These capabilities extend to mobile robots in the real world.",authors:{"Chenguang Huang":[],"Oier Mees":[],"Andy Zeng":[],"Wolfram Burgard":[]},affiliations:[],pdf:"/papers/2023/4.pdf"}),(0,b.Y)(E,{title:"Fully Automated Task Management for Generation, Execution, and Evaluation: A Framework for Fetch-and-Carry Tasks with Natural Language Instructions in Continuous Space",abstract:"This paper aims to develop a framework that enables a robot to execute tasks based on visual information, in response to natural language instructions for Fetch-and-Carry with Object Grounding (FCOG) tasks. Although there have been many frameworks, they usually rely on manually given instruction sentences. Therefore, evaluations have only been conducted with fixed tasks. Furthermore, many multimodal language understanding models for the benchmarks only consider discrete actions. To address the limitations, we propose a framework for the full automation of the generation, execution, and evaluation of FCOG tasks. In addition, we introduce an approach to solving the FCOG tasks by dividing them into four distinct subtasks.",authors:{"Motonari Kambara":[],"Komei Sugiura":[]},affiliations:[],pdf:"/papers/2023/6.pdf"}),(0,b.Y)(E,{title:"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",abstract:"In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. ",authors:{"Chan Hee Song":[],"Jiaman Wu":[],"Clayton B Washington":[],"Brian M. Sadler":[],"Wei-Lun Chao":[],"Yu Su":[]},affiliations:[],pdf:"/papers/2023/7.pdf"}),(0,b.Y)(E,{title:"Emergence of Implicit System Identification via Embodiment Randomization",abstract:"We show that embodiment randomization can produce visual navigation agents that are able to generalize to new embodiments in a zero-shot manner. Our embodiment randomization technique is simple and can easily be scaled for agents trained in simulation. Specifically, in training, we randomize various agent embodiment parameters such as height, radius, camera field-of-view, etc. Using the image-goal navigation task, we empirically find that single embodiment policies catastrophically fail to generalize to new embodiments, while embodiment randomized agents maintain strong performance. Through deeper analysis, we discover embodiment randomization produces agents that implicitly perform system identification.",authors:{"Pranav Putta":[],"Gunjan Aggarwal":[],"Roozbeh Mottaghi":[],"Dhruv Batra":[],"Naoki Harrison Yokoyama":[],"Joanne Truong":[],"Arjun Majumdar":[]},affiliations:[],pdf:"/papers/2023/8.pdf"}),(0,b.Y)(E,{title:"Predicting Motion Plans for Articulating Everyday Objects",abstract:"Mobile manipulation tasks such as opening a door, pulling open a drawer, or lifting a toilet lid require constrained motion of the end-effector under environmental and task constraints. This, coupled with partial information in novel environments, makes it challenging to employ classical motion planning approaches at test time. Our key insight is to cast it as a learning problem to leverage past experience of solving similar planning problems to directly predict motion plans for mobile manipulation tasks in novel situations at test time. To enable this, we develop a simulator, ArtObjSim, that simulates articulated objects placed in real scenes. We then introduce SeqIK+, a fast and flexible representation for motion plans. Finally, we learn models that use SeqIK+ to quickly predict motion plans for articulating novel objects at test time. Experimental evaluation shows improved speed and accuracy at generating motion plans than pure search-based methods and pure learning methods.",authors:{"Arjun Gupta":[],"Max Shepherd":[],"Saurabh Gupta":[]},affiliations:[],pdf:"/papers/2023/9.pdf"}),(0,b.Y)(E,{title:"SalsaBot: Towards a Robust and Generalizable Embodied Agent",abstract:"As embodied agents become more powerful, there arises a need for an agent to collaboratively solve tasks with humans. This paper introduces SalsaBot, an embodied agent designed for the Alexa Arena benchmark, which is a collaborative human-robot interaction benchmark. The primary aim of SalsaBot is to assist users in completing a game within a virtual environment by providing a consistent user-centric experience, which requires the agent to be capable of handling various types of user interactions. To ensure a great user experience, SalsaBot is equipped with robust macros, an explicit object memory, and a state-aware dialogue generation module. Our efforts and findings demonstrate that our SalsaBot is a robust interactive agent that can effectively collaborate with users.",authors:{"Chan Hee Song":[],"Jiaman Wu":[],"Ju-Seung Byun":[],"Zexin Xu":[],"Vardaan Pahuja":[],"Goonmeet Bajaj":[],"Samuel Stevens":[],"Ziru Chen":[],"Yu Su":[]},affiliations:[],pdf:"/papers/2023/10.pdf"}),(0,b.Y)(E,{title:"Exploiting Proximity-Aware Tasks for Embodied Social Navigation",abstract:"Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.",authors:{"Enrico Cancelli":[],"Tommaso Campari":[],"Luciano Serafini":[],"Angel X Chang":[],"Lamberto Ballan":[]},affiliations:[],pdf:"/papers/2023/11.pdf"}),(0,b.Y)(E,{title:"A Hypothetical Framework of Embodied Generalist Agent with Foundation Model Assistance",abstract:"Recent significant advancements in computer vision (CV) and natural language processing (NLP) have showcased the vital importance of leveraging prior knowledge obtained from extensive data for a generalist agent. However, there are limited explorations in utilizing internet-scale data to train embodied generalist agents. In this work, we propose a hypothetical framework that integrates the prior knowledge from foundation models into each component of the actor-critic algorithms for the generalist agents.",authors:{"Weirui Ye":[],"Yunsheng Zhang":[],"Xianfan Gu":[],"Yang Gao":[]},affiliations:[],pdf:"/papers/2023/12.pdf"}),(0,b.Y)(E,{title:"Situated Real-time Interaction with a Virtually Embodied Avatar",abstract:"Recent advances in large language model fine-tuning datasets and techniques have made them flourish as general dialogue-based assistants that are well-suited to strictly turn-based interactions. However, maintaining consistency in long-range, multi-turn dialogues remains a challenge with many applications restricting conversations to a short window. Current multi-modal vision-based interactions are also limited to turn-based interactions on a static sequence of tokenized images with VQA-style referential querying. In this work, we present an approach to performing real-time, vision-based dynamic interaction with an auto-regressive language model. Our approach enables long-range consistency through continual visual grounding of language model inputs. Grounding makes use of a winnowing mechanism to reduce a raw stream of pixels hierarchically, to a series of discrete events as conditioning variables for the language model. We present a novel dataset and benchmark for situated, visual interaction in the form of exercise coaching, and show that our approach can generate relevant and useful responses grounded in a real-time camera stream.",authors:{"Sunny Panchal":[],"Guillaume Berger":[],"Antoine Mercier":[],"Cornelius Böhm":[],"Florian Dietrichkeit":[],"Xuanlin Li":[],"Reza Pourreza":[],"Pulkit Madan":[],"Apratim Bhattacharyya":[],"Mingu Lee":[],"Mark Todorovich":[],"Ingo Bax":[],"Roland Memisevic":[]},affiliations:[],pdf:"/papers/2023/13.pdf"}),(0,b.Y)(E,{title:"When Learning Is Out of Reach, Reset: Generalization in Autonomous Visuomotor Reinforcement Learning",abstract:"Episodic training, where an agent's environment is reset to some initial condition after every success or failure, is the de facto standard when training embodied reinforcement learning (RL) agents. The underlying assumption that the environment can be easily reset is limiting both practically, as resets generally require human effort in the real world and can be computationally expensive in simulation, and philosophically, as we'd expect intelligent agents to be able to continuously learn without external intervention. Work in learning without any resets, i.e. Reset-Free RL (RF-RL), is very promising but is plagued by the problem of irreversible transitions (e.g. an object breaking or falling out of reach) which halt learning. Moreover, the limited state diversity and instrument setup encountered during RF-RL means that works studying RF-RL largely do not require their models to generalize to new environments. In this work, we instead look to minimize, rather than completely eliminate, resets while building visual agents that can meaningfully generalize. As studying generalization has previously not been a focus of benchmarks designed for RF-RL, we propose a new Stretch Pick-and-Place (Stretch-P&P) benchmark designed for evaluating generalizations across goals, cosmetic variations, and structural changes. Moreover, towards building performant reset-minimizing RL agents, we propose unsupervised metrics to detect irreversible transitions and a single-policy training mechanism to enable generalization. Our proposed approach significantly outperforms prior episodic, reset-free, and reset-minimizing approaches achieving higher success rates with fewer resets in Stretch-P&P and another popular RF-RL benchmark. Finally, we find that our proposed approach can dramatically reduce the number of resets required for training other embodied tasks, in particular for RoboTHOR ObjectNav we obtain higher success rates than episodic approaches using 99.97% fewer resets.",authors:{"Zichen Zhang":[],"Luca Weihs":[]},affiliations:[],pdf:"/papers/2023/14.pdf"}),(0,b.Y)(E,{title:"SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment",abstract:"This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat Simulator. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation.",authors:{"Tatiana Zemskova":[],"Margarita Kichik":[],"Dmitry Yudin":[],"Aleksandr Panov":[]},affiliations:[],pdf:"/papers/2023/16.pdf"}),(0,b.Y)(E,{title:"Dynamic-Resolution Model Learning for Object Pile Manipulation",abstract:"Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, almonds, corn, etc.",authors:{"Yixuan Wang":[],"Yunzhu Li":[],"Katherine Rose Driggs-Campbell":[],"Li Fei-Fei":[],"Jiajun Wu":[]},affiliations:[],pdf:"/papers/2023/17.pdf"}),(0,b.Y)(E,{title:"Reduce, Reuse, Recycle: Modular Multi-Object Navigation",abstract:"Our work focuses on the Multi-Object Navigation (MultiON) task, where an agent needs to navigate to multiple objects in a given sequence. We systematically investigate the inherent modularity of this task by dividing our approach to contain four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore its surroundings, and finally (d) a navigation module to move to identified target objects. We focus on the navigation and the exploration modules in this work. We show that we can effectively leverage a PointGoal navigation model in the MultiON task instead of learning to navigate from scratch. Our experiments show that a PointGoal agent-based navigation module outperforms analytical path planning on the MultiON task. We also compare exploration strategies and surprisingly find that a random exploration strategy significantly outperforms more advanced exploration methods. We additionally create MultiON 2.0, a new large-scale dataset as a test-bed for our approach.",authors:{"Sonia Raychaudhuri":[],"Tommaso Campari":[],"Unnat Jain":[],"Manolis Savva":[],"Angel X Chang":[]},affiliations:[],pdf:"/papers/2023/18.pdf"}),(0,b.Y)(E,{title:"Unordered Navigation to Multiple Semantic Targets in Novel Environments",abstract:"We consider the problem of unordered navigation to multiple objects in a novel environment. We define a multi-object navigation task which requires understanding of contextual semantic priors and reasoning over an optimal ordering of semantic targets, challenges missing from other multi-object task definitions and required by important motivating robotic scenarios. We develop a target-driven navigation objective trading off exploration and exploitation. To enable exploration, we explicitly predict unseen semantic regions and estimate uncertainty over those predictions, enabling us to solve the multi-object navigation problem by constructing a long horizon planning objective over an uncertain map. We demonstrate results for unordered navigation in the visually realistic environments of the Matterport3D dataset in the Habitat simulator. We find that our method leverages semantic relationships between objects in planning to allow exploitation of object co-occurrence.",authors:{"Bernadette Bucher":[],"Katrina Ashton":[],"Bo Wu":[],"Karl Schmeckpeper":[],"Siddharth Goel":[],"Nikolai Matni":[],"Georgios Georgakis":[],"Kostas Daniilidis":[]},affiliations:[],pdf:"/papers/2023/19.pdf"}),(0,b.Y)(E,{title:"EnvironAI: Extending AI Research into the Whole Environment",abstract:"This paper introduces Environment with AI (EnvironAI) as a complementary perspective to Embodied AI research. EnvironAI emphasizes the reciprocal relationship between AI, humans, and other elements, highlighting their dynamic co-construction within the environment. By integrating AI processing and human understanding, EnvironAI offers a comprehensive view that considers both individual elements and the entire system. A case study is presented to illustrate the practical application of EnvironAI, showcasing how the fusion of AI and human experience enhances our understanding of the big-picture metaphor. Overall, EnvironAI holds promise in deepening our insights into AI-human-environment interactions and fostering harmonious coexistence in the future.",authors:{"Jingyi Duan":[],"Song Tong":[],"Hongyi Shi":[],"Honghong Bai":[],"Xuefeng Liang":[],"Kaiping Peng":[]},affiliations:[],pdf:"/papers/2023/20.pdf"}),(0,b.Y)(E,{title:"Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos",abstract:"The analysis and use of egocentric videos for robotic tasks is made challenging by occlusion due to the hand and the visual mismatch between the human hand and a robot end-effector. In this sense, the human hand presents a nuisance. However, often hands also provide a valuable signal, e.g. the hand pose may suggest what kind of object is being held. In this work, we propose to extract a factored representation of the scene that separates the agent (human hand) and the environment. This alleviates both occlusion and mismatch while preserving the signal, thereby easing the design of models for downstream robotics tasks. At the heart of this factorization is our proposed Video Inpainting via Diffusion Model (VIDM) that leverages both a prior on real-world images (through a large-scale pre-trained diffusion model) and the appearance of the object in earlier frames of the video (through attention). Our experiments demonstrate the effectiveness of VIDM at improving inpainting quality on egocentric videos and the power of our factored representation for numerous tasks: from object detection to learning of reward functions, policies, and affordances from videos.",authors:{"Matthew Chang":[],"Aditya Prakash":[],"Saurabh Gupta":[]},affiliations:[],pdf:"/papers/2023/21.pdf"}),(0,b.Y)(E,{title:"Curriculum Learning via Task Selection for Embodied Navigation",abstract:"In this work, we study the use of ACL for training long-horizon embodied AI tasks with sparse rewards using RL. We focus on ACL methods which generate their curriculum via \\emph{task selection}, \\ie methods which select training tasks for the agent from a predefined dataset of existing tasks of varying complexity. We present a simple approach, \\textsc{ONACL}, which samples the next training task so that the predicted probability of the agent's success on task is near some threshold value.  Using \\textsc{ONACL} we present an empirical study of ACL on the ObjectGoal Navigation (\\textsc{ObjectNav}) task in the ProcTHOR and HM3D home environments. We find with a simple curriculum learning approach like \\textsc{ONACL}, the agent achieves a significant improvement in performance and sample efficiency. Surprisingly, however, we find that the commonly held belief that sparse reward training in HM3D obtains near 0\\% success is largely incorrect: if we simply add a sufficiently large number of `easy' episodes during policy training then (evaluation set) performance dramatically improves. We hypothesize this happens due to the emergence of an implicit curriculum during training and present an analysis supporting the claim. This suggests that, in some cases, curriculum learning approaches may simply be correcting for needlessly difficult training datasets.",authors:{"Ram Ramrakhya":[],"Dhruv Batra":[],"Aniruddha Kembhavi":[],"Luca Weihs":[]},affiliations:[],pdf:"/papers/2023/22.pdf"}),(0,b.Y)(E,{title:"DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training",abstract:"This paper focuses on the DialFRED task, which is the task of embodied instruction following in a setting where an agent can actively ask questions about the task. To address this task, we propose DialMAT. DialMAT introduces Moment-based Adversarial Training, which incorporates adversarial perturbations into the latent space of language, image, and action. Additionally, it introduces a crossmodal parallel feature extraction mechanism that applies foundation models to both language and image. We evaluated our model using a dataset constructed from the DialFRED dataset and demonstrated superior performance compared to the baseline method in terms of success rate and path weighted success rate. The model secured the top position in the DialFRED Challenge, which took place at the CVPR 2023 Embodied AI workshop.",authors:{"Kanta Kaneda":[],"Ryosuke Korekata":[],"Yuiga Wada":[],"Shunya Nagashima":[],"Motonari Kambara":[],"Yui Iioka":[],"Haruka Matsuo":[],"Yuto Imai":[],"Takayuki Nishimura":[],"Komei Sugiura":[]},affiliations:[],pdf:"/papers/2023/23.pdf"})];const z=e=>(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")},e.time),H=function(e){var a,t=e.length;for(;0!==t;)a=Math.floor(Math.random()*t),t--,[e[t],e[a]]=[e[a],e[t]];return e}((0,o.A)(Array(L.length).keys()));var O={name:"5ou1pc",styles:"width:15px;margin-right:5px"},F={name:"1guecoj",styles:"display:inline-block;border-radius:0px 10px 0px 10px;padding-left:10px;padding-right:10px;margin-top:3px;padding-top:3px;padding-bottom:4px;background-color:#4a154b;transition-duration:0.15s;color:white;&:hover{cursor:pointer;filter:contrast(1.25);}>span,>img{vertical-align:middle;}"};function W(){return(0,b.Y)("div",null,(0,b.Y)("a",{href:"//join.slack.com/t/embodied-aiworkshop/shared_invite/zt-s6amdv5c-gBZQZ7YSktrD_tMhQDjDfg",target:"_blank"},(0,b.Y)("div",{css:F},(0,b.Y)("img",{src:p.A,css:O})," ",(0,b.Y)("span",null,"Ask questions on ",(0,b.Y)("b",null,"Slack")))))}var j={name:"rzxmlp",styles:"display:grid;grid-gap:2%;grid-row-gap:20px;grid-template-columns:49% 49%;@media (max-width: 600px){grid-template-columns:100%;}"},N={name:"1azakc",styles:"text-align:center"},G={name:"jdiuzp",styles:"margin-top:25px;margin-bottom:50px"},B={name:"1qs5g6e",styles:"margin-left:0px;margin-top:20px"},V={name:"13ocxcu",styles:'width:130%;background-repeat:no-repeat;padding-top:70.25%;margin-top:-25px;margin-left:-15%;margin-bottom:-15px;background-image:url("/images/cvpr2023/cover-small.png");background-size:cover;background-position:center'};function q(e){let{data:a}=e;const[t,o]=s.useState(M());s.useEffect((()=>{const e=()=>o(M());return window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)}));const u=[{challenge:w.Habitat,key:"habitat-objectNav",task:"ObjectNav",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"HM3D Semantics",actionSpace:"Continuous",observations:"RGB-D, Localization",stochasticAcuation:"",winner:"SkillFusion (AIRI)"},{challenge:w.Habitat,key:"habitat-imageNav",task:"ImageNav",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"HM3D Semantics",actionSpace:"Continuous",observations:"RGB-D, Localization",stochasticAcuation:"",winner:"LQ"},{challenge:w["RxR-Habitat"],key:"rxr",task:"Vision-and-Language Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D",actionSpace:"Discrete",stochasticAcuation:"",winner:"The GridMM Team"},{challenge:w.MultiOn,key:"multion",task:"Multi-Object Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"HM3D Semantics",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:"",winner:""},{challenge:w.SoundSpaces,key:"soundspaces",task:"Audio Visual Navigation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D, Audio Waveform",actionSpace:"Discrete",stochasticAcuation:"",winner:"AK-lab-tokyotech"},{challenge:w.SoundSpaces,key:"soundspaces",task:"Active Audio Visual Source Separation",interactiveActions:"",simulationPlatform:"Habitat",sceneDataset:"Matterport3D",observations:"RGB-D, Audio Waveform",actionSpace:"Discrete",stochasticAcuation:"",winner:"AK-lab-tokyotech"},{challenge:w["Robotic Vision Scene Understanding"],key:"rvsu-2",task:"Semantic SLAM",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"Partially",winner:"Team SP"},{challenge:w["Robotic Vision Scene Understanding"],key:"rvsu",task:"Rearrangement (SCD)",interactiveActions:"",simulationPlatform:"Isaac Sim",sceneDataset:"Active Scene Understanding",observations:"RGB-D, Pose Data, Flatscan Laser",actionSpace:"Discrete",stochasticAcuation:"✓",winner:"MSC Lab"},{challenge:w["TDW-Transport"],key:"tdw",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"TDW",sceneDataset:"TDW",observations:"RGB-D, Metadata",actionSpace:"Discrete",stochasticAcuation:"✓",winner:""},{challenge:w["AI2-THOR Rearrangement"],key:"ai2thor-rearrangement",task:"Rearrangement",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB-D, Localization",stochasticAcuation:"",winner:"TIDEE"},{challenge:w["Language Interaction"],key:"language-interaction",task:"Instruction Following and Dialogue",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",actionSpace:"Discrete",observations:"RGB",stochasticAcuation:"",winner:"Yonsei VnL"},{challenge:w.DialFRED,key:"teach",task:"Vision-and-Dialogue Interaction",interactiveActions:"✓",simulationPlatform:"AI2-THOR",sceneDataset:"iTHOR",observations:"RGB",actionSpace:"Discrete",stochasticAcuation:"",winner:"Team Keio"},{challenge:w.ManiSkill,key:"maniskill",task:"Generalized Manipulation",interactiveActions:"✓",simulationPlatform:"SAPIEN",sceneDataset:"PartNet-Mobility, YCB, EGAD",observations:"RGB-D, Metadata",actionSpace:"Continuous",stochasticAcuation:"",winner:"GXU-LIPE"}],p=g().tz("2022-05-17 04:59","America/Los_Angeles"),y=g()(),k=g().duration(p.diff(y));Math.ceil(k.asHours()%24),Math.floor(k.asDays());return(0,b.Y)(c.A,{headerGradient:"linear-gradient(0deg, #1f2f3f, #100b0f)",headerStyle:(0,b.AH)("color:",d.A.dark.gold10,"!important;button{&:hover{color:",d.A.dark.gold9,"!important;}}","","",""),imageContent:{css:V},conference:"CVPR 2023",rightSide:(0,b.Y)(h.NT,{conference:"CVPR 2023",challengeData:Object.values(w)})},(0,b.Y)(l.wn,{title:"Overview"},(0,b.Y)("p",null,"Minds live in bodies, and bodies move through a changing world. The goal of embodied artificial intelligence is to create agents, such as robots, which learn to creatively solve challenging tasks requiring interaction with the environment. While this is a tall order, fantastic advances in deep learning and the increasing availability of large datasets like ImageNet have enabled superhuman performance on a variety of AI tasks previously thought intractable. Computer vision, speech recognition and natural language processing have experienced transformative revolutions at passive input-output tasks like language translation and image processing, and reinforcement learning has similarly achieved world-class performance at interactive tasks like games. These advances have supercharged embodied AI, enabling a growing collection of researchers to make rapid progress towards intelligent agents which can:"),(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"See"),": perceive their environment through vision or other senses."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Talk"),": hold a natural language dialog grounded in their environment."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Listen"),": understand and react to audio input anywhere in a scene."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Act"),": navigate and interact with their environment to accomplish goals."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Reason"),": consider and plan for the long-term consequences of their actions.")),(0,b.Y)("p",null,"The goal of the Embodied AI workshop is to bring together researchers from computer vision, language, graphics, and robotics to share and discuss the latest advances in embodied intelligent agents. This year's workshop will focus on the three themes of:",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"Foundation Models"),": Large pretrained models such as CLIP, ViLD and PaLI which enable few-shot and zero-shot performance on novel tasks."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Generalist Agents"),": Single learning methods for multiple tasks, such as RT-1, which enable models trained on one task to be expanded to novel tasks."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Sim to Real Transfer"),": Techniques which enable models trained in simulation to be deployed in the real world.")),"The Embodied AI 2023 workshop will be held in conjunction with"," ",(0,b.Y)("a",{href:"https://cvpr2023.thecvf.com/"},"CVPR 2023")," ","in Vancouver, British Columbia. It will feature a host of invited talks covering a variety of topics in Embodied AI, many exciting Embodied AI challenges, a poster session, and panel discussions. For more information on the Embodied AI Workshop series, see our"," ",(0,b.Y)("a",{href:"https://arxiv.org/abs/2210.06849"},"Retrospectives")," ","paper on the first three years of the workshop."),(0,b.Y)(R,{actionIdentifier:"1FAIpQLSeIZrn-tk7Oain2R8gc_Q0HzLMLQ9XXwqu3KecK_E5kALpiug",entryNumber:1834823104})),(0,b.Y)(l.wn,{title:"Timeline"},(0,b.Y)(r.A,{progressDot:!0,current:4,direction:"vertical"},(0,b.Y)(v,{title:"Workshop Announced",description:"March 15, 2023"}),(0,b.Y)(v,{title:"Paper Submission Deadline",description:"May 26, 2023 (Anywhere on Earth)"}),(0,b.Y)(v,{title:"Challenge Submission Deadlines",description:"May 2023. Check each challenge for the specific date."}),(0,b.Y)(v,{title:"Fourth Annual Embodied AI Workshop at CVPR",description:(0,b.Y)(s.Fragment,null,(0,b.Y)("a",{href:"https://cvpr2023.thecvf.com/Conferences/2023",target:"_blank"},"Vancouver Convention Center")," ",(0,b.Y)("br",null),"Monday, June 19, 2023",(0,b.Y)("br",null),"9:00 AM - 5:30 PM PT",(0,b.Y)("br",null),"East Ballroom A ",(0,b.Y)("br",null),(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")}))}),(0,b.Y)(v,{title:"Challenge Winners Announced",description:"June 19, 2023 at the workshop. Check each challenge for specifics."}))),(0,b.Y)(l.wn,{title:"Workshop Schedule"},(0,b.Y)("p",null,(0,b.Y)(m.A,{fluid:a.workshopLocation.childImageSharp.fluid,alt:"Workshop Location"})),"Embodied AI will be a ",(0,b.Y)("b",null,"hybrid")," workshop, with both in-person talks and streaming via zoom.",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"Workshop Talks: 9:00AM-5:30PM PT - East Ballroom A")),(0,b.Y)("li",null,(0,b.Y)("b",null,"Poster Session: NOON-1:20PM PT - West Exhibit Hall, Posters #123 - #148"))),"Zoom information is available on ",(0,b.Y)("a",{href:"https://cvpr2023.thecvf.com/virtual/2023/index.html"},"the CVPR virtual platform for registered attendees"),".",(0,b.Y)("br",null),"Remote and in-person attendees are welcome to as questions via Slack:",(0,b.Y)("br",null),(0,b.Y)(W,null),(0,b.Y)("br",null),(0,b.Y)("div",{css:B},(0,b.Y)(i.A,null,(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Workshop Introduction: Embodied AI"),(0,b.Y)("br",null),"East Ballroom A",(0,b.Y)("br",null),(0,b.Y)(z,{time:"9:00 - 9:10 AM PT"}),(0,b.Y)(f.Speaker,{organizations:["NVIDIA"],name:"Claudia Perez D'Arpino",fixedImg:a.claudia.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Navigation & Understanding Challenge Presentations"),(0,b.Y)("br",null),"(Habitat, MultiON, SoundSpaces, RxR-Habitat, RVSU)",(0,b.Y)("br",null),(0,b.Y)(z,{time:"9:10 - 10:00 AM PT"}),(0,b.Y)("ul",null,(0,b.Y)("li",null,"9:10: RxR-Habitat"),(0,b.Y)("li",null,"9:20: MultiOn"),(0,b.Y)("li",null,"9:30: SoundSpaces"),(0,b.Y)("li",null,"9:40: RVSU"),(0,b.Y)("li",null,"9:50: Habitat"))),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Navigation & Understanding Challenge Q&A Panel"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"10:00 - 10:30 AM PT"})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Embodied Navigation: "),(0,b.Y)("br",null),(0,b.Y)("i",null,"Robot Learning by Understanding Videos"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"10:30 - 11:00 AM PT"}),(0,b.Y)(f.Speaker,{organizations:["UIUC"],name:"Saurabh Gupta",fixedImg:a.saurabh.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Saurabh Gupta is an Assistant Professor in the ECE Department at UIUC. Before starting at UIUC in 2019, he received his Ph.D. from UC Berkeley in 2018 and spent the following year as a Research Scientist at Facebook AI Research in Pittsburgh. His research interests span computer vision, robotics, and machine learning, with a focus on building agents that can intelligently interact with the physical world around them. He received the President's Gold Medal at IIT Delhi in 2011, the Google Fellowship in Computer Vision in 2015, an Amazon Research Award in 2020, and an NSF CAREER Award in 2022. He has also won many challenges at leading computer vision conferences."),(0,b.Y)(T,{text:"True gains of machine learning in AI sub-fields such as computer vision and natural language processing have come about from the use of large-scale diverse datasets for learning. In this talk, I will discuss if and how we can leverage large-scale diverse data in the form of egocentric videos (first-person videos of humans conducting different tasks) to similarly scale up policy learning for robots. I will discuss the challenges this presents, and some of our initial efforts towards tackling them. In particular, I will describe work that extracts a) spatial common sense and b) an interactive understanding of objects from such videos."})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Robotics: "),(0,b.Y)("br",null),(0,b.Y)("i",null,"Embodied Reasoning Through Planning with Language and Vision Foundation Models"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"11:00 - 11:30 AM PT"}),(0,b.Y)(f.Speaker,{organizations:["Google"],name:"Fei Xia",fixedImg:a.fei.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Fei Xia is a Research Scientist at Google Research where he works on the Robotics team. He received his PhD degree from the Department of Electrical Engineering, Stanford University. He was co-advised by Silvio Savarese in SVL and Leonidas Guibas. His mission is to build intelligent embodied agents that can interact with complex and unstructured real-world environments, with applications to home robotics. He has been approaching this problem from 3 aspects: 1) Large scale and transferrable simulation for Robotics. 2) Learning algorithms for long-horizon tasks. 3) Combining geometric and semantic representation for environments. Most recently, He has been exploring using foundation models for robot decision making."),(0,b.Y)(T,{text:"Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could in principle be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack contextual grounding, which makes it difficult to leverage them for decision-making within a given real-world context. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide this grounding by means of pretrained behaviors, which are used to condition the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level tasks can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these tasks provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator."})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Generalist Agents: "),(0,b.Y)("br",null),(0,b.Y)("i",null,"Building Embodied Autonomous Agents with Multimodal Interaction"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"11:30 AM - 12 NOON PT"}),(0,b.Y)(f.Speaker,{organizations:["CMU"],name:"Ruslan Salakhutdinov",fixedImg:a.ruslan.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Russ Salakhutdinov is a UPMC Professor of Computer Science in the Department of Machine Learning at CMU. He received his PhD in computer science from the University of Toronto. After spending two post-doctoral years at MIT, he joined the University of Toronto and later moved to CMU. Russ's primary interests lie in deep learning, machine learning, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research, served as a director of AI research at Apple, served on the senior programme committee of several top-tier learning conferences including NeurIPS and ICML, was a program co-chair for ICML 2019, and will serve as a general chair for ICML 2024.  He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, a recipient of the Early Researcher Award, Google Faculty Award, and Nvidia's Pioneers of AI award."),(0,b.Y)(T,{text:"In this talk I will give an overview of our recent work on how we can design modular agents for visual navigation that can perform tasks specified by natural language instructions, perform efficient exploration and long-term planning, build and utilize 3D semantic maps, while generalizing across domains and tasks. In the second part of the talk, I will introduce a method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images, I will show that the model is able to achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase its interactive abilities."})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Accepted Papers Poster Session"),(0,b.Y)("br",null),"West Exhibit Hall - Posters #123 - #148.",(0,b.Y)("br",null),(0,b.Y)(z,{time:"12:00 NOON - 1:20 PM PT"})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Foundation Models: "),(0,b.Y)("br",null),(0,b.Y)("i",null,"Large Language Models for Solving Long-Horizon Robotic Manipulation Problems"),(0,b.Y)("br",null),"East Ballroom A",(0,b.Y)("br",null),(0,b.Y)(z,{time:"1:30 - 2:00 PM PT"}),(0,b.Y)(f.Speaker,{organizations:["Stanford"],name:"Jeannette Bohg",fixedImg:a.jeannette.childImageSharp.fixed,noMargin:!0}),(0,b.Y)(T,{text:"My long-term research goal is enable real robots to manipulate any kind of object such that they can perform many different tasks in a wide variety of application scenarios such as in our homes, in hospitals, warehouses, or factories. Many of these tasks will require long-horizon reasoning and sequencing of skills to achieve a goal state. In this talk, I will present our work on enabling long-horizon reasoning on real robots for a variety of different long-horizon tasks that can be solved by sequencing a large variety of composable skill primitives. I will specifically focus on the different ways Large Language Models (LLMs) can help with solving these long-horizon tasks. The first part of my talk will be on TidyBot, a robot for personalised household clean-up. One of the key challenges in robotic household cleanup is deciding where each item goes. People's preferences can vary greatly depending on personal taste or cultural background. One person might want shirts in the drawer, another might want them on the shelf. How can we infer these user preferences from only a handful of examples in a generalizable way? Our key insight: Summarization with LLMs is an effective way to achieve generalization in robotics. Given the generalised rules, I will then show how TidyBot then solves the long-horizon task of cleaning up a home. In the second part of my talk, I will focus on more complex long-horizon manipulation tasks that exhibit geometric dependencies between different skills in a sequence. In these tasks, the way a robot performs a certain skill will determine whether a follow-up skill in the sequence can be executed at all. I will present an approach called text2motion that utilises LLMs for task planning without the need for defining complex symbolic domains. And I will show how we can verify whether the plan that the LLM came up with is actually feasible. The basis for this verification is a library of learned skills and an approach for sequencing these skills to resolve geometric dependencies prevalent in long-horizon tasks."})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Sim to Real"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Toward Foundational Robot Manipulation Skills"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"2:00 - 2:30 PM PT"}),(0,b.Y)(f.Speaker,{organizations:["NVIDIA","U Washington"],name:"Dieter Fox",fixedImg:a.dieter.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,'Dieter Fox received his PhD degree from the University of Bonn, Germany. He is a professor in the Allen School of Computer Science & Engineering at the University of Washington, where he heads the UW Robotics and State Estimation Lab.  He is also Senior Director of Robotics Research at NVIDIA.  His research is in robotics and artificial intelligence, with a focus on learning and estimation applied to problems such as robot manipulation, planning, language grounding, and activity recognition. He has published more than 300 technical papers and is co-author of the textbook "Probabilistic Robotics". Dieter is a Fellow of the IEEE, ACM, and AAAI, and recipient of the IEEE RAS Pioneer Award and the IJCAI John McCarthy Award.'),(0,b.Y)(T,{text:"In this talk, I will discuss our ongoing efforts toward developing the models and generating the kind of data that might lead to foundational manipulation skills for robotics.  To generate large amounts of data, we sample many object rearrangement tasks in physically realistic simulation environments and apply task and motion planning to generate high quality solutions for them.  We will then train manipulation skills so that they can be used across a broad range of object rearrangement tasks in unknown, real-world environments.  We believe that such skills could provide the glue between generative AI reasoning and robust execution in the real world."})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Interaction & Rearrangement Challenge Presentations"),(0,b.Y)("br",null),"AI2-Rearrangement, ALFRED+TEACh, DialFRED, ManiSkill, TDW-Transport",(0,b.Y)("br",null),(0,b.Y)(z,{time:"2:30 - 3:30 PM PT"}),(0,b.Y)("ul",null,(0,b.Y)("li",null,"2:30: AI2-Rearrangement"),(0,b.Y)("li",null,"2:40: ALFRED+TEACh"),(0,b.Y)("li",null,"2:50: DialFRED"),(0,b.Y)("li",null,"3:00: ManiSkill"),(0,b.Y)("li",null,"3:10: TDW-Transport"),(0,b.Y)("li",null,"3:20: Break"))),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Interaction & Rearrangement Challenge Q&A Panel"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"3:30 - 4:00 PM PT"})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Talk - External Knowledge"),(0,b.Y)("br",null),(0,b.Y)("i",null,"From goals to grasps: Learning about action from people in video"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"4:00 - 4:30 PM PT"}),(0,b.Y)(f.Speaker,{organizations:["UT Austin"],name:"Kristen Grauman",fixedImg:a.kristen.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Kristen Grauman is a Professor in the Department of Computer Science at the University of Texas at Austin and a Research Director in Facebook AI Research (FAIR).  Her research in computer vision and machine learning focuses on video, visual recognition, and action for perception or embodied AI.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She is an IEEE Fellow, AAAI Fellow, Sloan Fellow, a Microsoft Research New Faculty Fellow, and a recipient of NSF CAREER and ONR Young Investigator awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the International Joint Conference on Artificial Intelligence (IJCAI), the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013.  She was inducted into the UT Academy of Distinguished Teachers in 2017.  She and her collaborators have been recognized with several Best Paper awards in computer vision, including a 2011 Marr Prize and a 2017 Helmholtz Prize (test of time award).  She served for six years as an Associate Editor-in-Chief for the Transactions on Pattern Analysis and Machine Intelligence (PAMI) and for ten years as an Editorial Board member for the International Journal of Computer Vision (IJCV).  She also served as a Program Chair of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2015 and a Program Chair of Neural Information Processing Systems (NeurIPS) 2018, and will serve as a Program Chair of the IEEE International Conference on Computer Vision (ICCV) 2023.")),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Invited Speaker Panel"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"4:30 - 5:30 PM PT"}),(0,b.Y)("br",null),(0,b.Y)(f.Speaker,{organizations:["Logical Robotics"],name:"Moderator - Anthony Francis",fixedImg:a.anthony.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(i.A.Item,null,(0,b.Y)("b",null,"Workshop Concludes"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"5:30 PM PT"}))))),(0,b.Y)(l.wn,{title:"Demos"},(0,b.Y)("p",null,"In association with the Embodied AI Workshop, Meta AI will present a demo of ",(0,b.Y)("b",null,"LSC: Language-guided Skill Coordination for Open-Vocabulary Mobile Pick-and-Place"),' in which a Boston Dynamics Spot will follow voice commands for object rearrangement such as "Find the plush in the table and place it in the case." The demo times for LSC include:'),(0,b.Y)("ul",null,(0,b.Y)("li",null,"Expo Meta AI Booth: ",(0,b.Y)("b",null,"Tue-Thu, June 20-22 11:00-5:00")),(0,b.Y)("li",null,"West Exhibit Hall Demo Area: ",(0,b.Y)("b",null,"Thu, June 22 10:00-18:00"))),(0,b.Y)("br",null),(0,b.Y)("p",null,(0,b.Y)("center",null,(0,b.Y)(m.A,{fluid:a.metaDemo.childImageSharp.fluid,alt:"Meta Demo"})))),(0,b.Y)(l.wn,{title:"Challenges"},(0,b.Y)("p",null,"The Embodied AI 2023 workshop is hosting many exciting challenges covering a wide range of topics such as rearrangement, visual navigation, vision-and-language, and audio-visual navigation. More details regarding data, submission instructions, and timelines can be found on the individual challenge websites."),(0,b.Y)("p",null,"The workshop organizers are awarding each first-place challenge winner $300 dollars, sponsored by Apple, Hello Robot and Logical Robotics."),(0,b.Y)("p",null,"Challenge winners will be given the opportunity to present a talk at the workshop. Since many challenges can be grouped into similar tasks, we encourage participants to submit models to more than 1 challenge. The table below describes, compares, and links each challenge."),(0,b.Y)(n.A,{scroll:{x:"1500px"},css:G,sticky:!0,columns:[{title:(0,b.Y)(s.Fragment,null,"Challenge"),dataIndex:"challenge",key:"challenge",fixed:t>650?"left":""},{title:(0,b.Y)(s.Fragment,null,"Task"),dataIndex:"task",key:"task",sorter:(e,a)=>e.task.localeCompare(a.task),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"2023 Winner"),dataIndex:"winner",key:"winner",sorter:(e,a)=>e.task.localeCompare(a.winner),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"Simulation Platform"),dataIndex:"simulationPlatform",key:"simulationPlatform",sorter:(e,a)=>e.simulationPlatform.localeCompare(a.simulationPlatform),sortDirections:["ascend","descend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Scene Dataset"),dataIndex:"sceneDataset",key:"sceneDataset",sorter:(e,a)=>e.sceneDataset.localeCompare(a.sceneDataset),sortDirections:["ascend","descend"],width:180},{title:(0,b.Y)(s.Fragment,null,"Observations"),key:"observations",dataIndex:"observations",sorter:(e,a)=>e.observations.localeCompare(a.observations),sortDirections:["ascend","descend"],width:170},{title:(0,b.Y)("div",{css:N},"Action Space"),key:"actionSpace",dataIndex:"actionSpace",sorter:(e,a)=>e.actionSpace.localeCompare(a.actionSpace),sortDirections:["ascend","descend"],width:165},{title:(0,b.Y)(s.Fragment,null,"Interactive Actions?"),dataIndex:"interactiveActions",key:"interactiveActions",sorter:(e,a)=>e.interactiveActions.localeCompare(a.interactiveActions),sortDirections:["descend","ascend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Stochastic Acuation?"),key:"stochasticAcuation",dataIndex:"stochasticAcuation",sorter:function(e,a){let t="✓"===e.stochasticAcuation?"Z":e.stochasticAcuation,n="✓"===a.stochasticAcuation?"Z":a.stochasticAcuation;return t.localeCompare(n)},sortDirections:["descend","ascend"]}],dataSource:u,pagination:!1})),(0,b.Y)(l.wn,{title:"Call for Papers"},(0,b.Y)("p",null,"We invite high-quality 2-page extended abstracts on embodied AI, especially in areas relevant to the themes of this year's workshop:",(0,b.Y)("ul",null,(0,b.Y)("li",null,"Foundation Models"),(0,b.Y)("li",null,"Generalist Agents"),(0,b.Y)("li",null,"Sim to Real Transfer")),"as well as themes related to embodied AI in general:",(0,b.Y)("ul",null,(0,b.Y)("li",null,"Simulation Environments"),(0,b.Y)("li",null,"Visual Navigation"),(0,b.Y)("li",null,"Rearrangement"),(0,b.Y)("li",null,"Embodied Question Answering"),(0,b.Y)("li",null,"Embodied Vision & Language")),"Accepted papers will be presented as posters or spotlight talks at the workshop. These papers will be made publicly available in a non-archival format, allowing future submission to archival journals or conferences. Paper submissions do not have to be anononymized. Per"," ",(0,b.Y)("a",{href:"https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines",target:"_blank"},"CVPR rules")," ","regarding workshop papers, at least one author must register for CVPR using an in-person registration."),(0,b.Y)(l.Wo,{title:"Submission"},(0,b.Y)("p",null,"The submission deadline is May 26th (",(0,b.Y)("a",{href:"//time.is/Anywhere_on_Earth"},"Anywhere on Earth"),"). Papers should be no longer than 2 pages (excluding references) and styled in the"," ",(0,b.Y)("a",{href:"https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines",target:"_blank"},"CVPR format"),".",(0,b.Y)("uL",null,(0,b.Y)("li",null,"Paper submissions have now CLOSED.")))),(0,b.Y)(l.Wo,{title:"Accepted Papers"},(0,b.Y)("p",null,(0,b.Y)("b",null,"Note.")," The order of the papers is randomized each time the page is refreshed."),(0,b.Y)("div",{css:j},H.map((e=>L[e]))))),(0,b.Y)(l.wn,{title:"Sponsors"},(0,b.Y)("p",null,"The Embodied AI 2023 Workshop is sponsored by the following organizations:"),(0,b.Y)("p",null,(0,b.Y)("center",null,(0,b.Y)("a",{href:"https://www.apple.com/"},(0,b.Y)("img",{src:"/images/sponsors/apple.svg",width:"100",alt:"Apple"})),(0,b.Y)("a",{href:"https://hello-robot.com/"},(0,b.Y)("img",{src:"/images/sponsors/hello-robot.png",width:"550",alt:"Hello Robot"})),(0,b.Y)("a",{href:"https://logicalrobotics.com/"},(0,b.Y)("img",{src:"/images/sponsors/logical-robotics.png",width:"400",alt:"Logical Robotics"}))))),(0,b.Y)(l.wn,{title:"Organizers"},"The Embodied AI 2023 workshop is a joint effort by a large set of researchers from a variety of organizations. They are listed below in alphabetical order.",(0,b.Y)(l.Wo,{title:"Organizing Committee"},(0,b.Y)(f.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2023.organizers.filter((e=>!0===e.oc)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a})),(0,b.Y)(l.Wo,{title:"Challenge Organizers"},(0,b.Y)(f.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2023.organizers.filter((e=>!0===e.challenge)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a})),(0,b.Y)(l.Wo,{title:"Scientific Advisory Board"},(0,b.Y)(f.OrganizerPics,{organizers:a.allSite.nodes[0].siteMetadata.cvpr2023.organizers.filter((e=>!0===e.sab)).sort(((e,a)=>e.name.localeCompare(a.name))),data:a}))))}}}]);
//# sourceMappingURL=cc945e9f-4d015c8d6275e50e809e.js.map