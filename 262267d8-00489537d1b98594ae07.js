"use strict";(self.webpackChunkembodied_ai_workshop=self.webpackChunkembodied_ai_workshop||[]).push([[654],{1275:function(e,i,n){n.r(i),n.d(i,{default:function(){return q}});n(3960);var a=n(1381),t=(n(1651),n(9304)),o=n(436),r=(n(9412),n(7262)),s=n(6540),l=n(6568),c=n(3020),d=n(2316),u=n(2158),h=n(4743),g=n.n(h),m=n(4333),p=n(853),f=n(7850),b=n(1013);const{Step:v}=r.A;const y={ARNOLD:(0,b.Y)("a",{href:"https://sites.google.com/view/arnoldchallenge/",target:"_blank"},"ARNOLD"),HAZARD:(0,b.Y)("a",{href:"https://embodied-agi.cs.umass.edu/hazard",target:"_blank"},"HAZARD"),ManiSkillViTac:(0,b.Y)("a",{href:"https://ai-workshops.github.io/maniskill-vitac-challenge-2025/",target:"_blank"},"ManiSkill-ViTac"),SMM:(0,b.Y)("a",{href:"https://smm-challenge.github.io/",target:"_blank"},"Social Mobile Manipulation")};var w={name:"14y6t8x",styles:"display:none!important"},Y={name:"99y604",styles:"margin-top:5px;color:#8c8c8c"},k={name:"rowx5j",styles:"border-radius:5px;box-shadow:0px 0px 2px 0px #2b4acb;display:inline-block;margin:auto;*{padding-top:3px;padding-bottom:5px;}"},A={name:"nl3i1y",styles:"vertical-align:middle;display:inline-block;margin-top:6px;margin-left:5px"},x={name:"13sa3rf",styles:'font-weight:bold;font-size:25px;color:"#2b4acb";vertical-align:middle;display:inline-block'},I={name:"pr10xp",styles:"margin-bottom:10px"},M={name:"uewfz3",styles:"text-align:center;margin-top:60px;margin-bottom:60px"};function S(e){const[i,n]=s.useState(!1),[a,t]=s.useState(!1),[o,r]=s.useState(""),l=/^(([^<>()[\]\\.,;:\s@"]+(\.[^<>()[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/.test(String(o).toLowerCase());return(0,b.Y)("div",{css:M},(0,b.Y)("form",{encType:"text/plain",action:l?"https://docs.google.com/forms/d/e/"+e.actionIdentifier+"/formResponse?usp=pp_url&entry."+e.entryNumber+"="+o:"",target:"hidden_iframe"+e.actionIdentifier,onSubmit:()=>!!l&&n(!0),method:"post"},(0,b.Y)("div",{css:I},(0,b.Y)("div",{css:x},"Sign Up for Updates"),(0,b.Y)("div",{css:A})),i?(0,b.Y)("div",null,"Thanks for signing up!"):(0,b.Y)(s.Fragment,null,(0,b.Y)("div",{css:k},(0,b.Y)("input",{type:"email",autoComplete:"off",placeholder:"email",name:"entry."+e.entryNumber,id:"entry."+e.entryNumber,onFocus:()=>t(!0),onBlur:()=>t(!1),onChange:e=>r(e.target.value),value:o,css:(0,b.AH)("background-color:transparent;transition-duration:0.3s;box-shadow:0px 0px 1px 2px ",a||l||""==o?"transparent":"#ff7875",";border:none;width:350px;@media (max-width: 500px){width:55vw;}border-radius:5px;padding-left:8px;","","","")}),(0,b.Y)("input",{type:l?"submit":"button",value:"Sign Up",onClick:()=>!!l,css:(0,b.AH)("background-color:transparent;border:none;font-weight:600;transition-duration:0.3s;color:",l?"#2b4acb":"#2b4acb88",";padding-top:3px;padding-right:12px;padding-left:10px;&:hover{cursor:",l?"pointer":"default",";}","","","")})),(0,b.Y)("div",{css:Y},"You can unsubscribe at any time."))),(0,b.Y)("iframe",{name:"hidden_iframe"+e.actionIdentifier,id:"hidden_iframe"+e.actionIdentifier,css:w}))}function D(){if("undefined"==typeof window)return 800;const{innerWidth:e}=window;return e}var C={name:"nkt64x",styles:"margin-right:10px"};function R(e){return(0,b.Y)("a",{href:e.url,target:"_blank",css:C},(0,b.Y)("div",{css:(0,b.AH)("display:inline-block;border:1px solid ",d.A.gray5,";background-color:",d.A.gray2,";padding-left:7px;padding-right:7px;border-radius:5px;transition-duration:0.15s;>span{vertical-align:middle;}&:hover{background-color:",d.A.gray4,";border:1px solid ",d.A.gray6,";}","","","")},(0,b.Y)("span",{css:(0,b.AH)("margin-left:5px;color:",d.A.gray10,";","","","")},e.text)))}function P(e){const[i,n]=s.useState(!1);let a;return a=-1===e.text.indexOf(" ",250)?(0,b.Y)("div",null,e.text):(0,b.Y)("div",null,i?e.text+" ":e.text.indexOf(". ")+2>250?e.text.slice(0,e.text.indexOf(". ")+2):e.text.slice(0,250)+"... ",(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>n((e=>!e))},"[",i?"Collapse":"Expand","]")),(0,b.Y)("div",{css:(0,b.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:25px;}","","","")},a)}var T={name:"4ozr74",styles:"position:absolute;bottom:10px;width:calc(100% - 40px);padding-top:5px"};function L(e){const[i,n]=s.useState(!1);let a;return a=-1===e.abstract.indexOf(" ",250)?(0,b.Y)("div",null,e.abstract):(0,b.Y)("div",null,i?e.abstract+" ":e.abstract.slice(0,e.abstract.indexOf(". ")+2),(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.light.blue6,";&:hover{cursor:pointer;}","","",""),onClick:()=>n((e=>!e))},"[",i?"Collapse":"Expand","]")),(0,b.Y)("div",{css:(0,b.AH)("padding:20px;background:",d.A.gray1,";border:1px solid ",d.A.gray5+"cc",";box-shadow:0px 0px 100px 0px ",d.A.gray4,";border-radius:0px;padding-bottom:45px;text-align:left;vertical-align:top;display:inline-block;position:relative;@media (min-width: 601px){min-height:250px;}","","","")},(0,b.Y)("a",{href:e.pdf,target:"_blank"},(0,b.Y)("div",{css:(0,b.AH)("font-weight:600;line-height:20px;color:",d.A.light.blue7,";font-size:15px;transition-duration:0.15s;&:hover{color:",d.A.light.blue6,";}","","","")},e.title)),(0,b.Y)("div",{css:(0,b.AH)("margin-bottom:8px;color:",d.A.gray8,";line-height:20px;font-size:13px;","","","")},Object.keys(e.authors).map(((i,n)=>(0,b.Y)(s.Fragment,null,(0,b.Y)("span",null,i),(0,b.Y)("sup",null),n!==Object.keys(e.authors).length-1?", ":"")))),a,(0,b.Y)("div",{css:T},(0,b.Y)(R,{text:"PDF",url:e.pdf}),e.poster?(0,b.Y)(R,{text:"Poster",url:e.poster}):(0,b.Y)(s.Fragment,null)))}let E=[(0,b.Y)(L,{title:"Real-Time Multimodal Processing for Interpreting Embodied Actions",abstract:"In this paper, we demonstrate how real-time integration of language with embodied gesture and action in a collaborative task enables the generation of AI agent interventions that result in ”positive friction”, or reflection, deliberation, and more mindful collaboration. Further, we demonstrate how the same framework can be adapted toward agent action generation for real-time task guidance.",authors:{"Hannah VanderHoeven":[],"Videep Venkatesha":[],"Abhijnan Nath":[],"Nikhil Krishnaswamy":[]},affiliations:[]}),(0,b.Y)(L,{title:"On the use of Pre-trained Visual Representations in Visuo-Motor Robot Learning",abstract:"The use of pre-trained visual representations (PVRs) in visuo-motor robot learning offers an alternative to training encoders from scratch but we discover that it faces challenges such as temporal entanglement and poor generalisation to minor scene changes. These issues hinder performance in tasks requiring temporal awareness and scene robustness. We address these limitations by: (1) augmenting PVR features with temporal perception and task completion signals to disentangle them over time, and (2) introducing a module that selectively attends to task-relevant local features, improving robustness in out-of-distribution scenes. Our approach, particularly effective for PVRs trained with masking objectives, shows significant performance gains.",authors:{"Nikolaos Tsagkas":[],"Andreas Sochopoulos":[],"Duolikun Danier":[],"Sethu Vijayakumar":[],"Chris Xiaoxuan Lu":[],"Oisin Mac Aodha":[]},affiliations:[]}),(0,b.Y)(L,{title:"H^3 DP: Triply‑Hierarchical Diffusion Policy for Visuomotor Learning",abstract:"We introduce Triply-Hierarchical Diffusion Policy (H^3DP), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering; (2) multi-scale visual representations; and (3) a hierarchically conditioned diffusion process. Extensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5}$% average relative improvement over baselines across $\\mathbf{44}$ simulation tasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.",authors:{"Yiyang Lu":[],"Yufeng Tian":[],"Zhecheng Yuan":[],"Xianbang Wang":[],"Pu Hua":[],"Zhengrong Xue":[],"Huazhe Xu":[]},affiliations:[]}),(0,b.Y)(L,{title:"Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI",abstract:"Diffusion models have shown remarkable flexibility for solving inverse problems without task-specific retraining. However, existing approaches like Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update per denoising step, limiting restoration fidelity and robustness—especially in embedded or out-of-distribution settings. In this work, we introduce a multi-step optimization strategy within each denoising timestep, significantly enhancing image quality, perceptual accuracy, and generalization. Our experiments on 4× super-resolution and Gaussian deblurring demonstrate that increasing the number of gradient updates per step improves LPIPS and PSNR, with minimal latency overhead. Notably, we validate this approach on a Jetson Orin Nano using degraded ImageNet and UAV123 aerial imagery, showing that MPGD—originally trained on face datasets—generalizes effectively to natural and aerial scenes. Our findings highlight MPGD’s potential as a lightweight, plug-and-play restoration module for real-time visual perception in embodied AI agents such as drones and mobile robots.",authors:{"Aditya Chakravarty":[]},affiliations:[]}),(0,b.Y)(L,{title:"EED: Embodied Environment Description through Robotic Visual Exploration",abstract:"The optimal way to convey information about a real environment to humans is through natural language descriptions. With the remarkable advancements in large language models and the field of Embodied AI in recent years, it has become possible for robots to autonomously navigate environments while recognizing and understanding their surroundings, much like humans do. In this paper, we propose a new Embodied AI task in which an autonomous mobile robot explores an environment and summarizes the entire environment in natural language. To properly evaluate this task, we use a crowdsourcing service to collect human-generated environment descriptions and construct a benchmark dataset. Additionally, the evaluation is conducted through a crowdsourcing service. Furthermore, we propose a baseline reinforcement learning method for the robot's environment exploration behavior to perform this task, demonstrating its superior performance compared to existing visual exploration methods.",authors:{"Kohei Matsumoto":[],"Asako Kanezaki":[]},affiliations:[]}),(0,b.Y)(L,{title:"R-EQA: Retrieval-Augmented Generation for Embodied Question Answering",abstract:"Embodied Question Answering (EQA) is a task where an agent explores its environment, gathers visual information and responds to natural language questions based on that information. The accuracy of the answer depends on which visual information is sampled for a given question. This study introduces R-EQA, a framework that uses Retrieval-Augmented Generation to evaluate the effectiveness of sampling semantically relevant visual information in the EQA setting. Experiments using the OpenEQA benchmark show that R-EQA achieves 10\\% higher performance compared to uniform sampling. This indicates that selective sampling of question-relevant information plays a critical role in improving answer quality in EQA.",authors:{"Hyobin Ong":[],"Minsu Jang":[]},affiliations:[]}),(0,b.Y)(L,{title:"Uncertainty Modeling in Autonomous Vehicle Trajectory Prediction: A Comprehensive Survey",abstract:"Agent Behavior prediction is a critical component in autonomous driving systems, requiring the modeling of inherent uncertainties in an agent's future motion. This survey provides a comprehensive overview of uncertainty quantification approaches in agent behavior prediction, categorizing them into three main paradigms: probabilistic distribution-based models, generative models, and heatmap-based representations. We analyze how these paradigms address different aspects of uncertainty - including intent ambiguity, control variations, and inter-agent interactions - and evaluate their performance across standard benchmarks. Our comparison reveals the trade-offs between model expressiveness, computational efficiency, and deployment practicality. We conclude by identifying promising research directions that could advance uncertainty-aware trajectory prediction, ultimately contributing to safer and more reliable autonomous driving systems in complex real-world environments.",authors:{"Siddharth Raina":[],"Jeshwanth Challagundla":[],"Mantek Singh":[]},affiliations:[]}),(0,b.Y)(L,{title:"ThinkSafe++: A Semantic Risk Score Framework for Safety-Aware Long-Horizon Planning",abstract:"ThinkSafe++ is a safety framework for long-horizon task planning in embodied agents. While LLMs can generate flexible plans, they often lack fine-grained safety reasoning, which may lead to hazardous behavior. Prior methods, such as SafeAgentBench, use binary filters that tend to over-reject and fail to distinguish between different types of risk. To address these limitations, ThinkSafe++ assigns continuous risk scores to each action step and leverages risk-type-specific distributions to guide filtering decisions. This enables more adaptive and semantically grounded safety control. We introduce two filtering strategies: (1) Global Risk-Score Filtering and (2) Risk-Type-Based Filtering. Experiments show that ThinkSafe++ improves safe task completion by 5.9 percentage points and reduces residual risk from 6.8% to 1.25%, achieving gains in both safety and efficiency.",authors:{"Yejin Jo":[],"Minsu Jang":[]},affiliations:[]}),(0,b.Y)(L,{title:"View-Imagination: Enhancing Visuomotor Control with Adaptive View Synthesis",abstract:"In robotic manipulation tasks, visuomotor control suffers from limited spatial understanding problems with limited camera installation and visual imperfections, such as occlusion. In this paper, we propose view-imagination, a novel framework with incorporating viewpoint policy. We train a dynamic scene NeRF and a learnable viewpoint policy, enabling the robot to generate a maximum value viewpoint to improve affordance. In experiments, we demonstrate that view-imagination outperforms across various training configurations.",authors:{"Dohyeok Lee":[],"Munkyung Kim":[],"Jung Min Lee":[],"Seungyub Han":[],"Jungwoo Lee":[]},affiliations:[]}),(0,b.Y)(L,{title:"Dynamics-Aligned Flow Matching Policy for Robot Learning",abstract:" Behavior cloning methods for robot learning suffer from poor generalization due to limited data support beyond expert demonstrations. While recent approaches leverage video prediction models to implicitly capture dynamics, they lack explicit action conditioning, leading to averaged predictions over actions that lose critical dynamics information. We propose a Dynamics-Aligned Flow Matching Policy that integrates dynamics predictions into policy learning through iterative flow generation. Our method introduces a novel architecture where policy and dynamics models share intermediate generation samples during training, enabling self-correction and improved generalization. Theoretical analysis demonstrates that conditioning on predicted dynamics leads to improved approximation to optimal actions, with empirical validation on Robomimic benchmarks.",authors:{"Dohyeok Lee":[],"Jung Min Lee":[],"Munkyung Kim":[],"Seokhun Ju":[],"Seungyub Han":[],"Jin Woo Koo":[],"Jungwoo Lee":[]},affiliations:[]}),(0,b.Y)(L,{title:"Data Augmentation in Diffusion Inversion Space",abstract:"Visual imitation learning methods have demonstrated strong performance and potential, but their generalization ability to unseen environments remains limited. Although data augmentation offers an effective solution to this problem, current approaches depend on complex preprocessing procedures, require substantial hardware resources, are time-consuming, and struggle to comprehensively account for all possible environments. Our goal is to develop a data augmentation method that is simple, efficient, plug-and-play, and incurs no additional computational overhead. Our core idea is that, instead of performing data augmentation in the raw image space, conducting it in the diffusion inversion space can significantly simplify the augmentation process — to the extent that inserting simple geometric shapes is sufficient to achieve broader coverage of environmental variations. We designed a simple industrial-style scenario experiment to preliminarily validate our idea.",authors:{"Junfeng Wei":[],"Rongsen Luo":[],"Ziming Cheng":[],"An Mo":[],"Chao Ji":[]},affiliations:[]}),(0,b.Y)(L,{title:"Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",abstract:"Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. One reason is that robot demonstration data used to train such policies often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during translation. For image translator training, we use only real-world robot play data from a single fixed camera but show that our method can generate diverse unseen viewpoints. We observe up to a 46% absolute improvement in manipulation success rates under viewpoint shift when we augment real data with our sim2real translated data.",authors:{"Jeremiah Coholich":[],"Justin Wit":[],"Zsolt Kira":[]},affiliations:[]}),(0,b.Y)(L,{title:"EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",abstract:" Sim-to-real transfer and personalization remains a core challenge in Embodied AI due to a trade-off between synthetic environments lacking realism and costly real-world captures. We present EmbodiedSplat, a method that personalizes policy training by reconstructing deployment environments using a mobile device and 3D Gaussian Splatting, enabling efficient fine-tuning in realistic scenes via Habitat-Sim. Our analysis of training strategies and reconstruction techniques shows that EmbodiedSplat achieves significant gains—improving real-world ImageNav success by 20–40% over pre-trained policies in an out-of-domain scene—and exhibits strong sim-to-real correlation (0.87–0.97). Code and data will be made public.",authors:{"Gunjan Chhablani":[],"Xiaomeng Ye":[],"Rynaa Grover":[],"Muhammad Zubair Irshad":[],"Zsolt Kira":[]},affiliations:[]}),(0,b.Y)(L,{title:"Benchmarking Arbitrary Natural Language Tasks in 3D Open Worlds",abstract:"3D-embodied autonomy toward arbitrary task outcomes is a long-standing goal in AI and Robotics. However, programmatically verifying arbitrary outcomes in open worlds is a challenge. This work proposes: (1) giving Minecraft agents the ability to capture screenshots as evidence for task completion and (2) having vision-language models (VLMs) evaluate these screenshots. We also present SemanticSteve, a high-level Minecraft skill library that includes a 'take screenshot' skill. We use an expert-annotated dataset of tricky task-screenshot pairs to evaluate the capabilities of GPT-4.1 in our proposed screenshot-evaluation role and find that it is indeed fit for the task. We make both the SemanticSteve library as well as the code and data for our experiments publicly available at https://github.com/sonnygeorge/semantic-steve.",authors:{"Sonny George":[],"Chris Sypherd":[],"Rocco Ahching":[],"Dylan Cashman":[]},affiliations:[]}),(0,b.Y)(L,{title:"BePo: Efficient Dual Representation for 3D Scene Understanding",abstract:"3D scene understanding fundamentally underlies autonomous systems that power a variety of important applications such as Autonomous Driving, Robotics, and AR/VR. Designing an expressive and compact scene representation is key to its goal of recovering detailed geometry and semantics of the surrounding environment from sensory images. Previous methods have adopted dense grids which are resource intensive and unable to handle diverse object scales. More recent efforts explore sparse points-based representations that are more object-centric but inefficient at modeling the entire scene. We present an efficient dual representation, termed BePo, that addresses these shortcomings and demonstrate the superiority of such representation through 3D occupancy prediction, a central task in 3D scene understanding.",authors:{"Yunxiao Shi":[],"Hong Cai":[],"Jisoo Jeong":[],"Yinhao Zhu":[],"Shizhong Han":[],"Amin Ansari":[],"Fatih Porikli":[]},affiliations:[]}),(0,b.Y)(L,{title:"LLM-Enhanced Rapid-Reflex Async-Reflect Framework for Real-Time Decision Making in Dynamically Changing Environments",abstract:" In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun evaluating agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates decision-making delays into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Framework (RRARF), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARF substantially outperforms existing baselines in latency-sensitive scenarios.",authors:{"Yangqing Zheng":[],"Shunqi Mao":[],"Dingxin Zhang":[],"Weidong Cai":[]},affiliations:[]}),(0,b.Y)(L,{title:"What matters in ImageNav: architecture, pre-training, sim settings, pose",abstract:"State-of-the-art image goal navigation methods either rely on dedicated image-matching or pre-training of vision modules on relative pose estimation or image reconstruction. Recent findings suggest that ImageNav can be solved by very low-capacity ResNet with channel-wise stacking and RL-training alone, without pre-training. These results raise interesting questions: can directional information, crucial to tackle ImageNav, be learned by RL alone, and by comparably simple architectures? In this study we investigate the effect of architectural choices like late fusion, channel stacking and cross-attention, and find that: (i) Pre-training and early patch-wise fusion are essential for strong performance, compared to late fusion. (ii) Success of recent frugal channel stacking architectures is likely due to a simulator setting allowing agents to slide along obstacles. Interestingly, capabilities learned in this regime can be transferred to realistic settings if the transfer includes weights of perception network. (iii) Navigation and (emerging) relative pose estimation performance are correlated.",authors:{"Gianluca Monaci":[],"Philippe Weinzaepfel":[],"Christian Wolf":[]},affiliations:[]}),(0,b.Y)(L,{title:"Object Retrieval-Guided Vision Language Modeling for Embodied Interaction",abstract:"Vision-language model (VLM)-based agents often struggle to name specific or unseen objects in hand-object interactions. We propose a zero-shot, real-time method that enhances VLM outputs by retrieving object features from a custom database and injecting prior knowledge into the captioning process during hand-object interactions. Our proposed approach enables users to guide an agent towards object-aware descriptions with task or job-specific objects, which are returned as speech output running in real time, as shown on GTEA and a smartphone-based user study with our collected dataset. The code is available on GitHub.",authors:{"Constantin Patsch":[],"Yuankai Wu":[],"Marsil Zakour":[],"Eckehard Steinbach":[]},affiliations:[]}),(0,b.Y)(L,{title:"MotIF: Motion Instruction Fine-tuning",abstract:"While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state -- e.g., if an apple is picked up -- many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs often use single frames, and thus cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an input of multiple frames, they still fail to correctly detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories with motion descriptions. MotIF assesses the success of robot motion given ask and motion instructions. Our model significantly outperforms state-of-the-art API-based single-frame VLMs and video LMs by at least twice in F1 score with high precision and recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in ranking trajectories on how they align with task and motion descriptions. Dataset, code, and checkpoints are in https://motif-1k.github.io/",authors:{"Minyoung Hwang":[],"Joey Hejna":[],"Dorsa Sadigh":[],"Yonatan Bisk":[]},affiliations:[]})];const z=e=>(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")},e.time),V=function(e){var i,n=e.length;for(;0!==n;)i=Math.floor(Math.random()*n),n--,[e[n],e[i]]=[e[i],e[n]];return e}((0,o.A)(Array(E.length).keys()));var W={name:"5ou1pc",styles:"width:15px;margin-right:5px"},H={name:"1guecoj",styles:"display:inline-block;border-radius:0px 10px 0px 10px;padding-left:10px;padding-right:10px;margin-top:3px;padding-top:3px;padding-bottom:4px;background-color:#4a154b;transition-duration:0.15s;color:white;&:hover{cursor:pointer;filter:contrast(1.25);}>span,>img{vertical-align:middle;}"};function F(){return(0,b.Y)("div",null,(0,b.Y)("a",{href:"//join.slack.com/t/embodied-aiworkshop/shared_invite/zt-s6amdv5c-gBZQZ7YSktrD_tMhQDjDfg",target:"_blank"},(0,b.Y)("div",{css:H},(0,b.Y)("img",{src:p.A,css:W})," ",(0,b.Y)("span",null,"Ask questions on ",(0,b.Y)("b",null,"Slack")))))}var N={name:"rzxmlp",styles:"display:grid;grid-gap:2%;grid-row-gap:20px;grid-template-columns:49% 49%;@media (max-width: 600px){grid-template-columns:100%;}"},O={name:"1azakc",styles:"text-align:center"},j={name:"jdiuzp",styles:"margin-top:25px;margin-bottom:50px"},G={name:"1qs5g6e",styles:"margin-left:0px;margin-top:20px"},B={name:"vkxy8j",styles:'width:130%;background-repeat:no-repeat;padding-top:70.25%;margin-top:0px;margin-left:-15%;margin-bottom:-15px;background-image:url("/images/cvpr2025/cover-small.png");background-size:cover;background-position:center'};function q(e){let{data:i}=e;const[n,o]=s.useState(D());s.useEffect((()=>{const e=()=>o(D());return window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)}));const h=[{challenge:y.ARNOLD,key:"arnold",task:"Language-Grounded Manipulation",interactiveActions:"✓",simulationPlatform:"Isaac Sim",sceneDataset:"Arnold Dataset",observations:"RGB-D, Proprioception",actionSpace:"Continuous",stochasticAcuation:"✓",winner:""},{challenge:y.HAZARD,key:"hazard",task:"Multi-Object Rescue",interactiveActions:"✓",simulationPlatform:"ThreeDWorld",sceneDataset:"HAZARD dataset",observations:"RGB-D, Temperature Sensors, Water Level",actionSpace:"Discrete",stochasticAcuation:"",winner:""},{challenge:y.ManiSkillViTac,key:"maniskill-vitac",task:"Vision-Tactile Fusion Manipulation",interactiveActions:"✓",simulationPlatform:"SAPIEN",sceneDataset:"Customized Scenarios",observations:"RGB-D, Proproioception, Tactile Signals",actionSpace:"Continuous",stochasticAcuation:"",winner:""},{challenge:y.SMM,key:"SMM",task:"Social Mobile Manipulation",interactiveActions:"✓",simulationPlatform:"Infinite World (based on Isaac Sim)",sceneDataset:"SMM Dataset",actionSpace:"Continuous",observations:"RGB-D",stochasticAcuation:"",winner:""}],p=g().tz("2022-05-17 04:59","America/Los_Angeles"),w=g()(),Y=g().duration(p.diff(w));Math.ceil(Y.asHours()%24),Math.floor(Y.asDays());return(0,b.Y)(c.A,{headerGradient:"linear-gradient(0deg, #e2d2b9, #153968)",headerStyle:(0,b.AH)("color:",d.A.dark.gold10,"!important;button{&:hover{color:",d.A.dark.gold9,"!important;}}","","",""),imageContent:{css:B},conference:"CVPR 2025 - Nashville",rightSide:(0,b.Y)(u.NT,{conference:"CVPR 2025",challengeData:Object.values(y)})},(0,b.Y)(l.wn,{title:"Attending"},(0,b.Y)("p",null,"The Embodied AI workshop will be held in-person at CVPR 2025 in Nashville, Tennessee on June 12th from 9 to 5 CDT:",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"In-Person:")," Workshop talks and panels will be held in room 101 D from 9-noon and 1:30-5 CDT."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Remote:"),"Zoom info for remote CVPR attendees can be found on our ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/virtual/2025/workshop/32284"},"official CVPR workshop page"),". Questions can be asked via Slack at",(0,b.Y)(F,null)),(0,b.Y)("li",null,(0,b.Y)("b",null,"Posters:")," Posters will be in ExHall D from 12:00 PM to 1:30 PM CDT at boards #140 to #169. Oral presentations will be in room 101 D from 3:30-4:00 PM CDT."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Printing:")," Information on ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/Conferences/2025/PosterPrintingInformation"},"poster printing")," is available on CVPR's website.")),"For late-breaking updates from CVPR, see the workshop's ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/virtual/2025/workshop/32284"},"CVPR page"),"."),(0,b.Y)("p",null,(0,b.Y)(m.A,{fluid:i.workshopLocation.childImageSharp.fluid,alt:"Workshop Location"}))),(0,b.Y)(l.wn,{title:"Overview"},(0,b.Y)("p",null,"Minds live in bodies, and bodies move through a changing world. The goal of embodied artificial intelligence is to create agents, such as robots, which learn to creatively solve challenging tasks requiring interaction with the environment. While this is a tall order, fantastic advances in deep learning and the increasing availability of large datasets like ImageNet have enabled superhuman performance on a variety of AI tasks previously thought intractable. Computer vision, speech recognition and natural language processing have experienced transformative revolutions at passive input-output tasks like language translation and image processing, and reinforcement learning has similarly achieved world-class performance at interactive tasks like games. These advances have supercharged embodied AI, enabling a growing collection of researchers to make rapid progress towards intelligent agents which can:"),(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"See"),": perceive their environment through vision or other senses."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Talk"),": hold a natural language dialog grounded in their environment."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Listen"),": understand and react to audio input anywhere in a scene."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Act"),": navigate and interact with their environment to accomplish goals."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Reason"),": consider and plan for the long-term consequences of their actions.")),(0,b.Y)("p",null,"The goal of the Embodied AI workshop is to bring together researchers from computer vision, language, graphics, and robotics to share and discuss the latest advances in embodied intelligent agents. EAI 2025’s overaching theme is ",(0,b.Y)("b",null,"Real-World Applications:")," creating embodied AI solutions that are deployed in real-world environments, ideally in the service of real-world tasks. Embodied AI agents are maturing, and the community should promote work that transfers this research out of simulation and laboratory environments into real-world settings. This umbrella theme is divided into four topics:",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"Embodied AI Solutions")," As embodied AI solutions become more powerful, we should demand of them that they solve more complex problems - particularly real-world problems outside of simulation and the laboratory. While scientific advances are of interest, we are actively seeking work that applies embodied AI to real-world industry applications."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Advances in Simulation")," Advances in simulation have enabled many embodied AI algorithms. Procedural simulation, parameterized simulation, differentiable simulation and world models are of interest, as are simulations based on the increasing numbers of large embodied datasets."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Generative Methods for Embodied AI")," Generative AI is becoming an increasingly important for embodied artificial intelligence research. Topics such as generative AI for simulation, generative AI for data generation, and generative AI for policies (e.g., diffusion policies and world models) are of great interest."),(0,b.Y)("li",null,(0,b.Y)("b",null,"Foundation Models")," Large-scale pretrained models adaptable to new tasks first came to the forefront in the domains of language, speech, and vision, but increasingly foundation models are being developed in robotics domains including action, perception, problem solving, and simulation. We invite both language model planning research that adapts existing models to embodied problems as well as embodied foundation models that are trained directly on embodied problems.")),"The Embodied AI 2025 workshop will be held in conjunction with"," ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/Conferences/2025"},"CVPR 2025")," ","in Nashville, Tennessee. It will feature a host of invited talks covering a variety of topics in Embodied AI, many exciting Embodied AI challenges, a poster session, and panel discussions. For more information on the Embodied AI Workshop series, see our"," ",(0,b.Y)("a",{href:"https://arxiv.org/abs/2210.06849"},"Retrospectives")," ","paper on the first three years of the workshop. For the latest updates, follow the Embodied AI Medium blog at"," ",(0,b.Y)("a",{href:"https://medium.com/embodied-artificial-intelligence"},"medium.com/embodied-artificial-intelligence"),"."),(0,b.Y)(S,{actionIdentifier:"1FAIpQLSeIZrn-tk7Oain2R8gc_Q0HzLMLQ9XXwqu3KecK_E5kALpiug",entryNumber:1834823104})),(0,b.Y)(l.wn,{title:"Timeline"},(0,b.Y)(r.A,{progressDot:!0,current:5,direction:"vertical"},(0,b.Y)(v,{title:"Workshop Announced",description:"March 31st, 2025"}),(0,b.Y)(v,{title:"Paper Submission Deadline",description:"CLOSED - Friday May 23rd, 2025"}),(0,b.Y)(v,{title:"Paper Notification Deadline",description:"CLOSED - Monday June 4nd, 2025"}),(0,b.Y)(v,{title:"Challenge Submission Deadlines",description:"May-June 2025. Check each challenge for the specific date."}),(0,b.Y)(v,{title:"Camera Ready Copy Deadline",description:"Tuesday June 11th, 2025"}),(0,b.Y)(v,{title:"Sixth Annual Embodied AI Workshop at CVPR",description:(0,b.Y)(s.Fragment,null,(0,b.Y)("a",{href:"https://cvpr.thecvf.com/Conferences/2025",target:"_blank"},"Nashville, Tennessee")," ",(0,b.Y)("br",null),"June 12, 2025",(0,b.Y)("br",null),(0,b.Y)("span",{css:(0,b.AH)("color:",d.A.gray7,";","","","")}))}),(0,b.Y)(v,{title:"Challenge Winners Announced",description:"At the workshop. Check each challenge for specifics."}))),(0,b.Y)(l.wn,{title:"Workshop Schedule"},"Embodied AI will be a ",(0,b.Y)("b",null,"hybrid")," workshop, with both in-person talks and streaming via zoom.",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"Workshop Talks: 9:00AM-5:00PM CDT - Room 101D")),(0,b.Y)("li",null,(0,b.Y)("b",null,"Poster Session: 12:00PM-1:30PM CDT - ExHall D boards #140 to #169"))),"Zoom information can be found for CVPR attendees on our ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/virtual/2025/workshop/32284"},"official CVPR workshop page"),".",(0,b.Y)("br",null),"Remote and in-person attendees are welcome to ask questions via Slack:",(0,b.Y)("br",null),(0,b.Y)(F,null),(0,b.Y)("br",null),(0,b.Y)("div",{css:G},(0,b.Y)(t.A,null,(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Workshop Introduction: Embodied AI"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"9:00 - 9:10 AM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["Logical Robotics"],name:"Anthony Francis",fixedImg:i.anthony.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Challenge Presentations - Winning Methods"),(0,b.Y)("br",null),"(ARNOLD, HAZARD, ManiSkill-ViTac, SMM)",(0,b.Y)("br",null),(0,b.Y)(z,{time:"9:10 - 10:00 AM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["CSIRO"],name:"Moderator - David Hall",fixedImg:i.davidH.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("ul",null,(0,b.Y)("li",null,"9:10: ",(0,b.Y)("a",{href:"https://sites.google.com/view/arnoldchallenge/"},"ARNOLD Challenge")),(0,b.Y)("li",null,"9:20: ",(0,b.Y)("a",{href:"https://embodied-agi.cs.umass.edu/hazard"},"HAZARD Challenge")),(0,b.Y)("li",null,"9:30: ",(0,b.Y)("a",{href:"https://ai-workshops.github.io/maniskill-vitac-challenge-2025/"},"ManiSkill-ViTac")),(0,b.Y)("li",null,"9:40: ",(0,b.Y)("a",{href:"https://smm-challenge.github.io/"},"SMM Challenge")))),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Challenge Q&A"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"10:00 - 10:30 AM CDT"}),(0,b.Y)("br",null),"Location: Room 101D"),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Embodied AI Applications"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: Learning from Humans with Vision and Touch"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"10:30 - 11:00 AM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["NYU"],name:"Lerrel Pinto",fixedImg:i.lerrelPinto.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Lerrel Pinto is an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. Lerrel runs the General-purpose Robotics and AI Lab (GRAIL) with the goal of getting robots to generalize and adapt in the messy world we live in."),(0,b.Y)(P,{text:"Abstract:  Despite rapid advances in robotics, robots still struggle to achieve the dexterity and adaptability of humans in real-world manipulation tasks. This talk explores how learning directly from humans—leveraging both vision and touch—can bridge this gap and unlock more robust, generalizable robot skills. I will present recent research that harnesses egocentric visual demonstrations, captured with wearable smart glasses, to extract robot-executable actions and enable closed-loop policy learning that generalizes across different robot morphologies and environments. Building on this, I will discuss new approaches for force-sensitive manipulation that combine vision-based hand pose estimation with tactile data from sensorized gloves, enabling robots to predict and control fine-grained contact forces with high precision. Finally, I will introduce AnySkin, a versatile and easily replaceable tactile sensor that supports cross-instance generalization of manipulation policies, making tactile learning scalable and practical for real-world deployment."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Foundation Models for Embodied AI "),(0,b.Y)("br",null),(0,b.Y)("i",null,"Towards Multimodal Embodied AI Agents that Can See, Talk and Act"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"11:00 - 11:30 AM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["Microsoft Research"],name:"Jianwei Yang",fixedImg:i.jianweiYang.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Jianwei Yang is a principal researcher in Deep Learning Group at Microsoft Research, Redmond, led by Jianfeng Gao. My research interests generally span in computer vision, multi-modality, and machine learning. Currently, I am focusing on building next-generation vision and multi-modal foundations."),(0,b.Y)(P,{text:"The development of multimodal AI agents marks a pivotal step toward creating systems capable of understanding, reasoning, and interacting with the world in human-like ways. Building such agents requires models that not only comprehend multi-sensory observations but also act adaptively to achieve goals within their environments. In this talk, I will present my research journey toward this grand goal across three key dimensions. First, I will explore how to bridge the gap between core vision understanding and multimodal learning through unified frameworks at various granularities. Next, I will discuss connecting vision-language models with large language models (LLMs) to create intelligent conversational systems. Finally, I will delve into recent advancements that extend multimodal LLMs into vision-language-action models, forming the foundation for general-purpose robotics policies. Together, these lead to an aspiration of building the next generation of multimodal and embodied AI agents capable of seeing, talking, and acting across diverse scenarios."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Simulation for Embodied AI"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: Geometry and Physics Bias in Embodied AI"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"11:30 AM - 12:00 PM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["Caltech"],name:"Jiayun (Peter) Wang",fixedImg:i.jiayunWang.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Jiayun (Peter) Wang is a postdoctoral researcher at the California Institute of Technology, working with Prof. Anima Anandkumar. He received his PhD from UC Berkeley in 2023, advised by Prof. Stella Yu. His research develops novel machine learning and computer vision methodologies that address challenges of data scarcity and computational cost, with real-world applications like healthcare. More information can be found at his website: https://pwang.pw/. "),(0,b.Y)(P,{text:"Abstract: Embodied AI demands agents that see the world with geometric fidelity, anticipate and interact with it with physical rigor. The talk will present a three-stage ladder—Perceive, Predict, Control—showing how carefully chosen geometry and physics biases enable that climb with minimal supervision. 1) Perceive. Pose-Aware Self-Supervised Learning learns semantic and geometric features from unlabeled videos. By regularizing along the agent’s own viewpoint trajectory, the network acquires a 3-D understanding without a single human label. 2) Predict and control. Controlling aerodynamic forces in turbulent conditions is crucial for UAV operation. We show AI enables realtime fluid flow prediction and turbulence control for wall friction reduction, which outperforms existing methods requiring expensive simulations of turbulent fluid dynamics. We further close the loop with FALCON, a model-based reinforcement learning framework for effective modeling and control of aerodynamic forces under turbulent flows. FALCON learns to control the underlying nonlinear dynamics when tested in the Caltech wind tunnel under highly turbulent conditions. Together, these works illustrate a unifying recipe: geometry grounds perception, physics grounds prediction and their composition unlocks fast, sample-efficient control."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Lunch / Accepted Papers Poster Session"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"12:00 PM - 1:30 PM CDT"}),(0,b.Y)("br",null),"Location: ExHall D",(0,b.Y)("ul",null,(0,b.Y)("li",null,"EAI's posters will be at boards #140 to #169."))),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Robotics and Embodied AI"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: The Ingredients for Efficient Robot Learning and Exploration"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"1:30 - 2:00 PM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["University of Cambridge"],name:"Rika Antonova",fixedImg:i.rikaAntonova.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Rika Antonova is an Associate Professor at the University of Cambridge. Her research interests include data-efficient reinforcement learning algorithms, robotics, active learning & exploration​. Earlier, Rika was a postdoctoral scholar at Stanford University upon receiving the Computing Innovation Fellowship from the US National Science Foundation. Rikacompleted her PhD at KTH, and earlier she obtained a research Master's degree from the Robotics Institute at Carnegie Mellon University. Before that, Rika was a senior software engineer at Google."),(0,b.Y)(P,{text:"Abstract: In this talk, I will outline the ingredients for enabling efficient robot learning. First, I will demonstrate how large vision-language models can enhance scene understanding and generalization, allowing robots to learn general rules from specific examples for handling everyday objects. Next, I will describe methods for leveraging equivariance to significantly reduce the amount of training data needed for learning from human demonstrations. Moving beyond demonstrations, I will discuss how simulation can enable robots to learn autonomously. I will describe the challenges and opportunities of aligning differentiable simulators with reality, and also introduce methods for facilitating reinforcement learning with 'black-box' simulators. To further expand robot capabilities we need adaptive hardware. I will demonstrate how differentiable simulation can be used for learning tool morphology to automatically adapt tools for robots. I will also outline experiments with new affordable and robust sensors. Finally, I will share plans for our new project on co-design of hardware and policy learning, which will leverage global optimization, rapid prototyping, and real-to-sim transfer to jointly search the vast space of hardware designs and reinforcement learning methods."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Foundation Models for Embodied AI"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: Large Behavior Models for Dexterous Manipulation"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"2:00 - 2:30 PM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["TRI"],name:"Rareș Ambruș",fixedImg:i.raresAmbrus.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Dr. Rareș Ambruș is a senior manager in the Large Behavior Models division at Toyota Research Institute (TRI). His research interests lie at the intersection of robotics, computer vision and machine learning with the aim of discovering visual representations for embodied applications in areas such as automated driving and robotics. Dr. Ambruș received his Ph.D. in 2017 from the Royal Institute of Technology (KTH), Sweden, focusing on self-supervised perception and mapping for mobile robots. He has more than 100 publications and patents at top AI venues covering fundamental topics in computer vision, machine learning and robotics."),(0,b.Y)(P,{text:"Abstract: Dexterous manipulation has seen tremendous progress in recent years, with imitation learning policies enabling successful performance of dexterous and hard-to-model tasks. Concurrently, scaling data and model size has led to the development of capable language and vision foundation models, motivating large-scale efforts to create general-purpose robot foundation models. In this talk, I'll describe our efforts at TRI at building Large Behavior Models: multi-task visuomotor policies for scalable dexterous manipulation. I’ll talk about some of our latest results when deploying these models for complex, dexterous and long horizon tasks and I’ll highlight the challenges associated with rigorous evaluation in the real world as well as some of the tools and experimental protocols we’ve built in order to measure progress in the real-world with confidence."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Generative AI for Embodied AI"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: World Models at Scale for Embodied Driving"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"2:30 - 3:00 PM CDT"}),(0,b.Y)("br",null),"Location: Room 101D",(0,b.Y)(f.Speaker,{organizations:["Wayve"],name:"Nikhil Mohan",fixedImg:i.nikhilMohan.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Nikhil Mohan is a Lead Scientist at Wayve, where he focuses on leveraging data-driven techniques for simulation in autonomous driving. His work spans Neural Radiance Fields (NeRFs), Gaussian Splatting, and generative models, emphasizing their application to improve and evaluate Wayve’s AI Driver performance. Before turning his attention to simulation, Nikhil led Wayve’s production driving team, where they shipped research prototypes into the production system. Prior to joining Wayve, he earned his Master’s degree at Carnegie Mellon University, concentrating in machine learning and signal processing."),(0,b.Y)(P,{text:"Abstract: Nikhil's talk will focus on using World Models to produce data at scale for Embodied AI in the context of self driving."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Talk - Generative AI for Embodied AI"),(0,b.Y)("br",null),(0,b.Y)("i",null,"Title: Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"3:00 - 3:30 PM CDT"}),(0,b.Y)(f.Speaker,{organizations:["NVIDIA"],name:"Huan Ling",fixedImg:i.huanLing.childImageSharp.fixed,noMargin:!0}),(0,b.Y)("p",null,"Bio: Huan Ling is a Senior Research Scientist at NVIDIA’s Spatial Intelligence (TorontoAI) Lab. His research focuses on developing foundational generative models that enable realistic and controllable environments—spanning video synthesis,  3D/4D scene generation and reconstruction. His work aims for building scalable systems that support real world applications.  Huan’s research has been featured at top conferences such as CVPR and NeurIPS, and he actively collaborates across disciplines to advance the frontier of generative AI for real-world applications. He has contributed to the development and large-scale training of video foundation model products, including NVIDIA-COSMOS and COSMOS-Drive-Dreams, which enable high-fidelity, controllable video generation for physicalAI related scenarios like autonomous driving."),(0,b.Y)(P,{text:"Abstract: Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive-Dreams, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our model weights through the NVIDIA’s Cosmos platform, pipeline toolkit, and a synthetic dataset which consists of 79,880 clips."})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Accepted Paper Highlights"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"3:30 - 4:00 PM CDT"}),(0,b.Y)("br",null),(0,b.Y)("ul",null,(0,b.Y)("li",null,"#2: On the use of Pre-trained Visual Representations in Visuo-Motor Robot Learning"),(0,b.Y)("li",null,"#6: R-EQA: Retrieval-Augmented Generation for Embodied Question Answering"),(0,b.Y)("li",null,"#7: Uncertainty Modeling in Autonomous Vehicle Trajectory Prediction: A Comprehensive Survey"),(0,b.Y)("li",null,"#15: Benchmarking Arbitrary Natural Language Tasks in 3D Open Worlds"),(0,b.Y)("li",null,"#19: What matters in ImageNav: architecture, pre-training, sim settings, pose"),(0,b.Y)("li",null,"#23: MotIF: Motion Instruction Fine-tuning")),(0,b.Y)(f.Speaker,{organizations:["CSIRO"],name:"Moderator - David Hall",fixedImg:i.davidH.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Invited Speaker Panel"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"4:00 - 4:30 PM CDT"}),(0,b.Y)("br",null),(0,b.Y)(f.Speaker,{organizations:["Logical Robotics"],name:"Moderator - Anthony Francis",fixedImg:i.anthony.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Debate on the Future of Embodied AI"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"4:30 - 5:00 PM CDT"}),(0,b.Y)("br",null),(0,b.Y)(f.Speaker,{organizations:["Logical Robotics"],name:"Moderator - Anthony Francis",fixedImg:i.anthony.childImageSharp.fixed,noMargin:!0})),(0,b.Y)(t.A.Item,null,(0,b.Y)("b",null,"Workshop Concludes"),(0,b.Y)("br",null),(0,b.Y)(z,{time:"5:00 PM CDT"}))))),(0,b.Y)(l.wn,{title:"Sponsor Events"},(0,b.Y)("p",null,(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("b",null,"NVIDIA:")," Check the ",(0,b.Y)("a",{href:"https://www.nvidia.com/en-us/events/cvpr/",target:"_blank"},"NVIDIA event page")," "," for the full list of events sponsored by NVIDIA at CVPR. Also, remember to checkout the ",(0,b.Y)("a",{href:"https://events.nvidia.com/nvcvprresearchercelebration",target:"_blank"},"NVIDIA party!")," "))),(0,b.Y)("br",null)),(0,b.Y)(l.wn,{title:"Challenges"},(0,b.Y)("p",null,"The Embodied AI 2025 workshop is hosting many exciting challenges covering a wide range of topics. More details regarding data, submission instructions, and timelines can be found on the individual challenge websites."),(0,b.Y)("p",null,"The workshop organizers will award each first-prize challenge winner a cash prize, sponsored by Logical Robotics and our other sponsors."),(0,b.Y)("p",null,"Challenge winners may be given the opportunity to present during their challenge's presentation at the the workshop. Since many challenges can be grouped into similar tasks, we encourage participants to submit models to more than 1 challenge. The table below describes, compares, and links each challenge."),(0,b.Y)(a.A,{scroll:{x:"1500px"},css:j,sticky:!0,columns:[{title:(0,b.Y)(s.Fragment,null,"Challenge"),dataIndex:"challenge",key:"challenge",fixed:n>650?"left":""},{title:(0,b.Y)(s.Fragment,null,"Task"),dataIndex:"task",key:"task",sorter:(e,i)=>e.task.localeCompare(i.task),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"2024 Winner"),dataIndex:"winner",key:"winner",sorter:(e,i)=>e.task.localeCompare(i.winner),sortDirections:["ascend","descend"]},{title:(0,b.Y)(s.Fragment,null,"Simulation Platform"),dataIndex:"simulationPlatform",key:"simulationPlatform",sorter:(e,i)=>e.simulationPlatform.localeCompare(i.simulationPlatform),sortDirections:["ascend","descend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Scene Dataset"),dataIndex:"sceneDataset",key:"sceneDataset",sorter:(e,i)=>e.sceneDataset.localeCompare(i.sceneDataset),sortDirections:["ascend","descend"],width:180},{title:(0,b.Y)(s.Fragment,null,"Observations"),key:"observations",dataIndex:"observations",sorter:(e,i)=>e.observations.localeCompare(i.observations),sortDirections:["ascend","descend"],width:170},{title:(0,b.Y)("div",{css:O},"Action Space"),key:"actionSpace",dataIndex:"actionSpace",sorter:(e,i)=>e.actionSpace.localeCompare(i.actionSpace),sortDirections:["ascend","descend"],width:165},{title:(0,b.Y)(s.Fragment,null,"Interactive Actions?"),dataIndex:"interactiveActions",key:"interactiveActions",sorter:(e,i)=>e.interactiveActions.localeCompare(i.interactiveActions),sortDirections:["descend","ascend"],width:200},{title:(0,b.Y)(s.Fragment,null,"Stochastic Acuation?"),key:"stochasticAcuation",dataIndex:"stochasticAcuation",sorter:function(e,i){let n="✓"===e.stochasticAcuation?"Z":e.stochasticAcuation,a="✓"===i.stochasticAcuation?"Z":i.stochasticAcuation;return n.localeCompare(a)},sortDirections:["descend","ascend"]}],dataSource:h,pagination:!1})),(0,b.Y)(l.wn,{title:"Call for Papers"},(0,b.Y)("p",null,"We invite high-quality 2-page extended abstracts on embodied AI, especially in areas relevant to the themes of this year's workshop:",(0,b.Y)("ul",null,(0,b.Y)("li",null,"Embodied AI Solutions"),(0,b.Y)("li",null,"Advances in Simulation"),(0,b.Y)("li",null,"Generative Methods for Embodied AI"),(0,b.Y)("li",null,"Foundation Models")),"as well as themes related to embodied AI in general:",(0,b.Y)("ul",null,(0,b.Y)("li",null,"Visual Navigation"),(0,b.Y)("li",null,"Embodied Mobile Manipulation"),(0,b.Y)("li",null,"Embodied Question Answering"),(0,b.Y)("li",null,"Embodied Vision & Language"),(0,b.Y)("li",null,"Language Model Planning")),"Accepted papers will be presented as posters or spotlight talks at the workshop. These papers will be made publicly available in a non-archival format, allowing future submission to archival journals or conferences. Paper submissions do not have to be anononymized. Per"," ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines",target:"_blank"},"CVPR rules")," ","regarding workshop papers, at least one author must register for CVPR using an in-person registration."),(0,b.Y)(l.Wo,{title:"Submission"},(0,b.Y)("p",null,"The submission deadline CLOSED on ",(0,b.Y)("b",null,"May 23rd")," ( Anywhere on Earth - for clarity, 2025/05/24 00:01 in GMT as computed by OpenReview). Papers should be no longer than 2 pages (excluding references) and styled in the"," ",(0,b.Y)("a",{href:"https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines",target:"_blank"},"CVPR format"),".",(0,b.Y)("ul",null,(0,b.Y)("li",null,(0,b.Y)("a",{href:"https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/EAI"},"Paper submissions are CLOSED as of May 23rd, 2025.")),(0,b.Y)("li",null,"Notifications were sent on June 4th, 2025."),(0,b.Y)("li",null,"Camera-ready copies of accepted papers are due by June 11th, 2025.")))),(0,b.Y)(l.Wo,{title:"Accepted Papers"},(0,b.Y)("p",null,(0,b.Y)("b",null,"Note.")," The order of the papers is randomized each time the page is refreshed."),(0,b.Y)("div",{css:N},V.map((e=>E[e]))))),(0,b.Y)(l.wn,{title:"Sponsors"},(0,b.Y)("p",null,"The Embodied AI 2025 Workshop is sponsored by the following organizations:"),(0,b.Y)("p",{style:{display:"flex",alignItems:"center",justifyContent:"space-between"}},(0,b.Y)("a",{href:"https://logicalrobotics.com/"},(0,b.Y)("img",{src:"/images/sponsors/logical-robotics.png",height:"60",alt:"Logical Robotics"})),(0,b.Y)("a",{href:"https://microsoft.com/"},(0,b.Y)("img",{src:"/images/sponsors/microsoft-logo.png",height:"200",alt:"Microsoft"})),(0,b.Y)("a",{href:"https://www.nvidia.com/"},(0,b.Y)("img",{src:"/images/sponsors/nvidia.svg",height:"75",alt:"NVIDIA"})),(0,b.Y)("a",{href:"https://wayve.ai/"},(0,b.Y)("img",{src:"/images/sponsors/wayve.webp",height:"65",alt:"Wayve"})))),(0,b.Y)(l.wn,{title:"Organizers"},"The Embodied AI 2025 workshop is a joint effort by a large set of researchers from a variety of organizations. Each year, a set of lead organizers takes point coordinating with the CVPR conference, backed up by a large team of workshop organizers, challenge organizers, and scientific advisors.",(0,b.Y)(l.Wo,{title:"Lead Organizers"},(0,b.Y)(f.OrganizerPics,{organizers:i.allSite.nodes[0].siteMetadata.cvpr2025.organizers.filter((e=>!0===e.lo)).sort(((e,i)=>e.name.localeCompare(i.name))),data:i})),(0,b.Y)(l.Wo,{title:"Organizing Committee"},(0,b.Y)(f.OrganizerPics,{organizers:i.allSite.nodes[0].siteMetadata.cvpr2025.organizers.filter((e=>!0===e.oc&&!1===e.lo)).sort(((e,i)=>e.name.localeCompare(i.name))),data:i})),(0,b.Y)(l.Wo,{title:"Challenge Organizers"},(0,b.Y)(f.OrganizerPics,{organizers:i.allSite.nodes[0].siteMetadata.cvpr2025.organizers.filter((e=>!0===e.challenge)).sort(((e,i)=>e.name.localeCompare(i.name))),data:i})),(0,b.Y)(l.Wo,{title:"Scientific Advisory Board"},(0,b.Y)(f.OrganizerPics,{organizers:i.allSite.nodes[0].siteMetadata.cvpr2025.organizers.filter((e=>!0===e.sab)).sort(((e,i)=>e.name.localeCompare(i.name))),data:i}))))}}}]);
//# sourceMappingURL=262267d8-00489537d1b98594ae07.js.map